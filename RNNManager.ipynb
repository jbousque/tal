{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Licenses\n",
    "GloVe\n",
    "Public Domain Dedication and License v1.0 whose full text can be found at: http://www.opendatacommons.org/licenses/pddl/1.0/\n",
    "\n",
    "Facebookresearch / FastText words embeddings\n",
    "https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "@article{bojanowski2016enriching,\n",
    "  title={Enriching Word Vectors with Subword Information},\n",
    "  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n",
    "  journal={arXiv preprint arXiv:1607.04606},\n",
    "  year={2016}\n",
    "}\n",
    "\n",
    "License Creative Commons Attribution-Share-Alike License 3.0 (https://creativecommons.org/licenses/by-sa/3.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display:none\">\n",
       "                <audio onended=\"this.parentNode.removeChild(this)\" controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"http://www.soundjay.com/button/beep-07.wav\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              </div>"
      ],
      "text/plain": [
       "<__main__.InvisibleAudio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optional - plays a sound when a cell completed\n",
    "# note: for any reason this should be executed after keras imports\n",
    "\n",
    "from time import time\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "class InvisibleAudio(Audio):\n",
    "    def _repr_html_(self):\n",
    "        audio = super()._repr_html_()\n",
    "        audio = audio.replace('<audio', f'<audio onended=\"this.parentNode.removeChild(this)\"')\n",
    "        return f'<div style=\"display:none\">{audio}</div>'\n",
    "\n",
    "class Beeper:\n",
    "\n",
    "    def __init__(self, threshold, **audio_kwargs):\n",
    "        self.threshold = threshold\n",
    "        self.start_time = None    # time in sec, or None\n",
    "        self.audio = audio_kwargs\n",
    "\n",
    "    def pre_execute(self):\n",
    "        if not self.start_time:\n",
    "            self.start_time = time()\n",
    "\n",
    "    def post_execute(self):\n",
    "        end_time = time()\n",
    "        if self.start_time and end_time - self.start_time > self.threshold:\n",
    "            audio = InvisibleAudio(**self.audio, autoplay=True)\n",
    "            display(audio)\n",
    "        self.start_time = None\n",
    "\n",
    "\n",
    "beeper = Beeper(5, url='http://www.soundjay.com/button/beep-07.wav')\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.events.register('pre_execute', beeper.pre_execute)\n",
    "ipython.events.register('post_execute', beeper.post_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class DataManager:\n",
    "    \n",
    "    root_dir_ = '.'\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.root_dir_ = os.getcwd()\n",
    "        \n",
    "    def load_dummy_data(self):\n",
    "        \"\"\"\n",
    "        This method makes available some dummy training data.\n",
    "        \"\"\"\n",
    "        X = self.pickle_load(\"data/fr_X.pkl\")\n",
    "        Y = self.pickle_load(\"data/fr_Y.pkl\")\n",
    "        vocab_mots = self.pickle_load(\"data/fr_vocab_mots.pkl\")\n",
    "        vocab_pdd = self.pickle_load(\"data/fr_vocab_pdd.pkl\")\n",
    "        vocab_liaisons = self.pickle_load(\"data/fr_vocab_liaisons.pkl\")\n",
    "        return X, Y, vocab_mots, vocab_pdd, vocab_liaisons\n",
    "        \n",
    "    def load_dummy_data_2(self):\n",
    "        \"\"\"\n",
    "        This method makes available some dummy training data.\n",
    "        \"\"\"\n",
    "        data = self.pickle_load(\"data/f1_fr_project_ok_bool.pkl\")\n",
    "        return data['X'], data['Y'], data['vocab_mots'], data['vocab_pdd'], data['vocab_liaisons'] \n",
    "    \n",
    "    def load_data(self, phase='train', lang='fr', featureset='f1'):\n",
    "        \"\"\"\n",
    "        Loads a dataset for a specific lang and feature set, and phase (train/dev/test).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        phase: str\n",
    "            'train', 'dev' or 'test'\n",
    "            \n",
    "        lang: str\n",
    "        \n",
    "        featureset: str\n",
    "            'f1', 'f2' or 'f3'\n",
    "        \n",
    "        \"\"\"\n",
    "        name = \"{featureset}_{lang}-{phase}\".format(lang=lang, featureset=featureset, phase=phase)\n",
    "        fname = os.path.join('data', name)\n",
    "        data = self.pickle_load(fname)\n",
    "        if data:\n",
    "            vocabs = {}\n",
    "            X = np.array(data['X'])\n",
    "            Y = np.array(data['Y'])\n",
    "            vocabs['WORDS'] = data['vocab_mots']\n",
    "            vocabs['POS'] = data['vocab_pdd']\n",
    "            vocabs['LABELS'] = data['vocab_liaisons']\n",
    "            if isinstance(vocabs['WORDS'], np.ndarray):\n",
    "                vocabs['WORDS'] = vocabs['WORDS'].tolist()\n",
    "            if isinstance(vocabs['POS'], np.ndarray):\n",
    "                vocabs['POS'] = vocabs['POS'].tolist()\n",
    "            if isinstance(vocabs['LABELS'], np.ndarray):\n",
    "                vocabs['LABELS'] = vocabs['LABELS'].tolist()\n",
    "                \n",
    "            if featureset == 'f2' or featureset == 'f3':\n",
    "                vocabs['MORPHO'] = data['vocab_morpho']\n",
    "                vocabs['LEMMA'] = data['vocab_lemma']\n",
    "                if isinstance(vocabs['MORPHO'], np.ndarray):\n",
    "                    vocabs['MORPHO'] = vocabs['MORPHO'].tolist()\n",
    "                if isinstance(vocabs['LEMMA'], np.ndarray):\n",
    "                    vocabs['LEMMA'] = vocabs['LEMMA'].tolist()\n",
    "                       \n",
    "            print(\"load_data: loaded X = \", X.shape, \", Y = \", Y.shape, \", vocabs = \", \n",
    "                  (''.join(\"{key} ({len}), \".format(key=k, len=len(vocabs[k])) for k in vocabs.keys())))\n",
    "            return X, Y, vocabs\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def merge_vocabs(self, vocab1, vocab2, data, columns, verbose=False):\n",
    "        \"\"\"\n",
    "        Merges vocab2 into vocab1, updating accordingly words references in data columns.\n",
    "        For all words in vocab2 with index idx_vocab2:\n",
    "        - if it exists in vocab1, then words features of data in columns refering to idx_vocab2 will be replaced by idx_vocab1\n",
    "        - if it does not exist in vocab1, it will be appended to vocab1 then replacement will be done as above in data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        vocab1: list(str)\n",
    "            A list of words from original vocabulary.\n",
    "                        \n",
    "        vocab2: list(str)\n",
    "            A list of words from vocabulary to be merged into vocab1 (used to build X2).\n",
    "            \n",
    "        data: array\n",
    "            Array containing the words indices to be updated by merge of vocabularies.\n",
    "            Usually this should be an array with samples as first dimension (rows), then columns for features.\n",
    "\n",
    "        columns: tuple(int)\n",
    "            Indices of columns to be updated in data.\n",
    "            \n",
    "        Returns:\n",
    "        \n",
    "        vocab1: list(str)\n",
    "            A new vocabulary with missing words appended to vocab1.\n",
    "            Note: vocab1 is also update in place, meaning this function modified original vocab1.\n",
    "        \n",
    "        ! data is updated in place.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        vocab1_ = vocab1.copy()\n",
    "        data_ = data.copy()\n",
    "        \n",
    "        for idx_vocab2, w in enumerate(vocab2):\n",
    "            # treat vocabs\n",
    "            if w in vocab1:\n",
    "                if verbose:\n",
    "                    print(\"word [{i}]={w} found in vocab1 at {j}\".format(i=idx_vocab2, w=w, j=vocab1.index(w)))\n",
    "                idx_vocab1 = vocab1.index(w)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"word [{i}]={w} not found in vocab1\".format(i=idx_vocab2, w=w))\n",
    "                vocab1.append(w)\n",
    "                idx_vocab1 = len(vocab1) - 1\n",
    "                \n",
    "            # replace all references in data\n",
    "            for i in range(len(data_)):\n",
    "                if len(data_.shape) == 1:\n",
    "                    if data_[i] == idx_vocab2:\n",
    "                        if verbose:\n",
    "                            print(\"Replacing word [{i}]= {idx}, {w} index {fro} to {to}\"\n",
    "                                    .format(i=i, idx=idx_vocab2, w=vocab2[idx_vocab2], \n",
    "                                            fro=idx_vocab2, to=idx_vocab1))\n",
    "                        data[i] = idx_vocab1\n",
    "                        #Replacing word [796]= 1, LEFT_advcl index 1 to 2\n",
    "                        \n",
    "                else:\n",
    "                    for idx, j in enumerate(columns):\n",
    "                        if data_[i][j] == idx_vocab2:\n",
    "                            if verbose:\n",
    "                                print(\"Replacing word [{i},{j}]= {idx}, {w} index {fro} to {to}\"\n",
    "                                      .format(i=i, j=j, idx=idx_vocab2, w=vocab2[idx_vocab2], \n",
    "                                              fro=idx_vocab2, to=idx_vocab1))\n",
    "                            data[i][j] = idx_vocab1\n",
    "\n",
    "        return vocab1, data\n",
    "        \n",
    "\n",
    "    # some utilities for saving results\n",
    "    def safe_pickle_dump(self, filename, obj):\n",
    "        \"\"\"\n",
    "        Serializes an object to file, creating directory structure if it does not exist.\n",
    "        \"\"\"\n",
    "        name = filename\n",
    "        print(\"pickle.dump \"+name)\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(name), exist_ok=True)\n",
    "            f = open(name,\"wb\")\n",
    "            pickle.dump(obj,f)\n",
    "            f.close()\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "        return True\n",
    "    \n",
    "    def pickle_load(self, filename):\n",
    "        \"\"\"\n",
    "        Deserialize an object from a file created with pickle.dump.\n",
    "        Returns False if this failed.\n",
    "        \"\"\"\n",
    "        name = filename\n",
    "        print(\"pickle.load \"+name)\n",
    "        try:\n",
    "            f = open(name,\"rb\")\n",
    "            obj = pickle.load(f)\n",
    "            f.close()\n",
    "            return obj\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "      \n",
    "        return None\n",
    "        \n",
    "    def load_embeddings(self, lang, type='fasttext'):\n",
    "        \"\"\"\n",
    "        Loads an embeddings file depending on its type and language.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        type: str\n",
    "            Only \"fasttext\" is supported.\n",
    "            \n",
    "        lang: str\n",
    "            See load_fasttext_embeddings(lang)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.load_fasttext_embeddings(lang)\n",
    "        \n",
    "    def load_fasttext_embeddings(self, lang):\n",
    "        \"\"\"\n",
    "        Loads a fasttext embedding, chosen depending on lang parameter provided.\n",
    "        File expected as root_dir_/data/embeddings/facebookresearch/wiki.{lang}.vec\n",
    "        (or as root_dir_/cache/wiki.{lang}.vec.pkl if already loaded once)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        lang: str\n",
    "            One of 'fr', 'ja', 'en', 'nl' (or additional ones depending on embeddings present on disk).\n",
    "        \n",
    "        \"\"\"\n",
    "        data_dict = {}\n",
    "        \n",
    "        pickle_fname = \"wiki.{lang}.vec.pkl\".format(lang=lang)\n",
    "        pickle_ffname = os.path.join(self.root_dir_, 'cache', pickle_fname)\n",
    "        \n",
    "        if os.path.isfile(pickle_ffname):\n",
    "            data_dict = self.pickle_load(pickle_ffname)\n",
    "            print(\"Embedding for {lang} loaded from {fname}\".format(lang=lang, fname=pickle_ffname))\n",
    "        else: \n",
    "            fname = \"wiki.{lang}.vec\".format(lang=lang)\n",
    "            data_file = os.path.join(self.root_dir_, 'data', 'embeddings', 'facebookresearch', fname)\n",
    "        \n",
    "            fin = io.open(data_file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "            n, d = map(int, fin.readline().split())\n",
    "            \n",
    "            for line in fin:\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                data_dict[tokens[0]] = list(map(float, tokens[1:]))\n",
    "            print(\"Embedding for {lang} loaded from {fname}\".format(lang=lang, fname=data_file))\n",
    "            # save embeddings as array format to improve speed next time\n",
    "            self.safe_pickle_dump(pickle_ffname, data_dict)\n",
    "\n",
    "        print(\"Read \", len(data_dict), \" words vectors\")\n",
    "        return data_dict\n",
    "    \n",
    "    def get_words_to_match_for_embeddings(self, word):\n",
    "        \"\"\"\n",
    "        Returns a list with same word, word lowercased, word lowercased and dashes removed, \n",
    "        then all this plus \\xa0 removed.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        word: str\n",
    "            Word to be transformed.\n",
    "            \n",
    "        Returns: list\n",
    "            List of transformed word forms.\n",
    "        \"\"\"\n",
    "        return [word, word.lower(), word.lower().replace('-', ''), word.lower().replace('-', '').replace('\\\\xa0', ' ')]\n",
    "    \n",
    "    def align_embeddings(self, vocab, embeddings, augment_vocab=True, max_vocab_size=-1):\n",
    "        \"\"\"\n",
    "        Generates aligned embeddings from original embeddings, and a vocabulary of words.\n",
    "        Words from vocabulary may not exist in original embeddings, in this case a random vector is generated.\n",
    "        Words matching is done as (by priority) : exact match, then case insensitively, then with dash ('-') removed.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        vocab: array\n",
    "            An array containing each word in the vocabulary.\n",
    "            \n",
    "        embeddings: dict\n",
    "            A dict object with words as keys and their embeddings (as a vector array) as values.\n",
    "            \n",
    "        augment_vocab: boolean\n",
    "            If True, then all words from embeddings not existing in vocab, are appended to vocab, after\n",
    "            alignment is done.\n",
    "            \n",
    "        max_vocab_size: int\n",
    "            Maximum length of resulting vocab if augment_vocab is True and if max_vocab_size < original vocab length.\n",
    "            If -1 then there is no limit.\n",
    "            Note: resulting vocab size can't be < original vocab size, whatever the value of max_vocab_size.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        aligned_embeddings: list\n",
    "            An array containing:\n",
    "          - for words from vocab found in embeddings, the corresponding embedding at same index as in vocab.\n",
    "          - for words not found in embeddings, a random vector, at same index as in vocab.\n",
    "          - if augment_vocab is True, all remaining words from embeddings (not found in vocab) are added after\n",
    "            len(vocab)\n",
    "        \n",
    "        words_not_found: list\n",
    "            An array containing indices (based on vocab) of words not found in embeddings and replaced by random\n",
    "            values.\n",
    "            \n",
    "        words_matched: list\n",
    "            An array of strings of words based on vocab, as they were matched in embeddings.\n",
    "            For example if lowercased word from vocab was matched, then lowercase version of this word will be found\n",
    "            in this table (whereas the original case sensitive word will remain as is in vocab array)\n",
    "        \n",
    "        \"\"\"\n",
    "        dim_embedding = len(embeddings[list(embeddings.keys())[0]]) # find length from value of 'first' key\n",
    "        \n",
    "        print(\"align_embeddings: aligning embeddings ({elen},{edim}) with vocab ({vlen}) using at most {max}\"\n",
    "             .format(elen=len(embeddings), edim=dim_embedding, vlen=len(vocab), max=max_vocab_size))\n",
    "        \n",
    "        cur_size = len(vocab) # to avoid computing vocab len at each iteration\n",
    "        \n",
    "        # first append missing embeddings to vocab, if required, to limit unknown words\n",
    "        if augment_vocab:\n",
    "            for embedding_word in enumerate(embeddings.keys()):\n",
    "                # adds missing word only up to max vocab size, if used\n",
    "                if max_vocab_size is not -1 and cur_size > max_vocab_size:\n",
    "                    break\n",
    "                elif embedding_word not in vocab:\n",
    "                    vocab.append(embedding_word)\n",
    "                    cur_size + cur_size + 1\n",
    "        \n",
    "            print(\"align_embeddings: new vocab size : \", len(vocab))\n",
    "        \n",
    "        aligned_embeddings = np.zeros((len(vocab), dim_embedding))\n",
    "        words_not_found = []\n",
    "        words_matched = [None] * len(vocab)\n",
    "        for idx_mot, mot in enumerate(vocab):\n",
    "            words_to_match = self.get_words_to_match_for_embeddings(mot)\n",
    "            for word_to_match in words_to_match:\n",
    "                if word_to_match in embeddings.keys():\n",
    "                    aligned_embeddings[idx_mot] = embeddings[word_to_match]\n",
    "                    words_matched[idx_mot] = word_to_match\n",
    "                    break\n",
    "            if words_matched[idx_mot] is None:\n",
    "                words_not_found.append(idx_mot)\n",
    "                words_matched[idx_mot] = mot\n",
    "                aligned_embeddings[idx_mot] = np.random.rand(dim_embedding)\n",
    "        \n",
    "        print(\"aligned_embeddings new embeddings shape {shap}, words not found {wnf}, words found {wf}\"\n",
    "             .format(shap=aligned_embeddings.shape, wnf=len(words_not_found), wf=len(words_matched)))\n",
    "        return aligned_embeddings, words_not_found, words_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation, Concatenate, Embedding, concatenate, Flatten, Dropout\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class RNNManager:\n",
    "    \n",
    "    UNKNOWN_WORD = '<UNK>' #@TODO handle unknown word (apax)\n",
    "    \n",
    "    MAX_VOCAB_SIZE = -1\n",
    "    \n",
    "    models_ = {}\n",
    "    networks_ = {}\n",
    "    path_ = '.'\n",
    "    \n",
    "    embeddings_ = None\n",
    "    embeddings_for_words_ = None\n",
    "    embeddings_for_lemmas_ = None\n",
    "    \n",
    "    dm_ = DataManager()\n",
    "    \n",
    "    def __init__(self, path='.', max_vocab_size=-1):\n",
    "        \"\"\"\n",
    "        Class to handle neural networks for TAL - TBP AE purpose.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        path: str\n",
    "            Root path to find files (path/.), scripts (path/.), cache (path/cache), embeddings (path/data/embeddings/...) ...\n",
    "            Current path by default.\n",
    "            \n",
    "        max_vocab_size: int\n",
    "            Maximum length of a vocabulary - hence for an embedding matrix used in a network.\n",
    "            This can be set to limit amount of memory used by vocabs and embeddings.\n",
    "            In practice, original pre-trained embeddings vectors are truncated up to this length, considering\n",
    "            that embeddings used (fasttext) are ordered from most frequent to least frequent word.\n",
    "            -1 (default value) means no limitation.\n",
    "        \n",
    "        \"\"\"\n",
    "        if path != '.':\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.path_ = path\n",
    "        self.MAX_VOCAB_SIZE = max_vocab_size\n",
    "        self.dm_ = DataManager()\n",
    "        \n",
    "    def get_model(self, model_name):\n",
    "        \"\"\"\n",
    "        Returns an existing TAL model, or None.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            Name of the model.\n",
    "        \"\"\"\n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        return self.models_[model_name]\n",
    "        \n",
    "    def create_model(self, model_name, lang, featureset, use_forms, vocabs, vocabs_dev, vocabs_test, embeddings_file=None):\n",
    "        \"\"\"\n",
    "        Creates a new TAL model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            Name of this model.\n",
    "            \n",
    "        lang: str\n",
    "            'fr', 'ja', 'nl', 'en'\n",
    "            \n",
    "        featureset: str\n",
    "            'f1', 'f2' or 'f3'\n",
    "            \n",
    "        add_forms: boolean\n",
    "            If true then both words forms are added on top of the features.\n",
    "            \n",
    "        vocabs: dict\n",
    "            Vocabs for learning task, with keys 'WORDS', 'POS', 'MORPHO' and/or 'LEMMA' (or 'LABELS' for targets).\n",
    "        \n",
    "        \"\"\"\n",
    "        model = {'name': model_name,\n",
    "                'lang': lang,\n",
    "                'featureset': featureset,\n",
    "                'use_forms': use_forms,\n",
    "                'vocabs': vocabs,\n",
    "                'vocabs_dev': vocabs_dev,\n",
    "                'vocabs_test': vocabs_test,\n",
    "                'embeddings_file': embeddings_file}\n",
    "        if model_name not in self.models_.keys():\n",
    "            self.models_[model_name] = model\n",
    "            \n",
    "        return model\n",
    "            \n",
    "    def remove_model(self, model_name):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if model_name in self.models_.keys():\n",
    "            return self.models_.pop(model_name, None)\n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if model_name in self.models_:\n",
    "            self.dm_.safe_pickle_dump(os.path.join(self.path_, model_name + '-model.pkl'), self.models_[model_name])\n",
    "        else:\n",
    "            print(\"Model \" + model_name + \" not found\")\n",
    "            \n",
    "    def load_model(self, model_fname):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        model_conf = self.dm_.pickle_load(os.path.join(self.path_, model_name, '-model.pkl'))\n",
    "        if model:\n",
    "            self.create_model(model_name=model['name'], lang=model['lang'], featureset=model['featureset'], \n",
    "                              vocabs=model['vocabs'])\n",
    "            return \n",
    "            \n",
    "        \n",
    "    def create_network(self, network_name, model_name, nb_classes, dropout=False):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        language: 'fr', 'ja', 'nl', 'en'\n",
    "            For french, japanese, dutch, or english. \n",
    "        \n",
    "        featureset: 'f1', 'f2', 'f3'\n",
    "            \n",
    "        \"\"\"\n",
    "        net_model = None\n",
    "        \n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        \n",
    "        model = self.models_[model_name]\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model['use_forms']\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        \n",
    "        net_model = Model()\n",
    "                       \n",
    "        #if embeddings != None:\n",
    "            \n",
    "        embedsw = self.embeddings_for_words_\n",
    "        embedsl = self.embeddings_for_lemmas_\n",
    "        \n",
    "        if embedsw is not None:\n",
    "            dim_embeddings = embedsw.shape[1]\n",
    "        else:\n",
    "            dim_embeddings = 300\n",
    "        \n",
    "        if use_forms:\n",
    "            if embedsw is not None:\n",
    "            \n",
    "                if len(vocabs['WORDS']) != embedsw.shape[0]:\n",
    "                    print(\"Words vocab size {v} must equal embeddings length {e}\".format(v=len(vocabs['WORDS']), \n",
    "                                                                               e=embedsw.shape[0]))\n",
    "                    return None\n",
    "            \n",
    "                # Pretrained Embedding layer for 2 words\n",
    "                input_word1 = Input(shape=(1,), dtype='int32', name='word_input_1')\n",
    "                embeddings_w1 = Embedding(\n",
    "                    input_dim=len(vocabs['WORDS']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    weights=[embedsw], \n",
    "                    input_length=1)(input_word1)\n",
    "                embeddings_w1 = Flatten()(embeddings_w1)\n",
    "                \n",
    "                # Embedding layer for second word\n",
    "                input_word2 = Input(shape=(1,), dtype='int32', name='word_input_2')\n",
    "                embeddings_w2 = Embedding(input_dim=len(vocabs['WORDS']), \n",
    "                                     output_dim=dim_embeddings, \n",
    "                                     weights=[embedsw], \n",
    "                                     input_length=1)(input_word2)\n",
    "                #embeddings_2 = embeddings_layer(input_word2) # sharing weights between both words embeddings\n",
    "                embeddings_w2 = Flatten()(embeddings_w2)\n",
    "            \n",
    "            else:\n",
    "                # Embedding layer for 2 words\n",
    "                input_word1 = Input(shape=(1,), dtype='int32', name='word_input_1')\n",
    "                embeddings_w1 = Embedding(\n",
    "                    input_dim=len(vocabs['WORDS']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    input_length=1)(input_word1)\n",
    "                embeddings_w1 = Flatten()(embeddings_w1)\n",
    "                \n",
    "                # Embedding layer for second word\n",
    "                input_word2 = Input(shape=(1,), dtype='int32', name='word_input_2')\n",
    "                embeddings_w2 = Embedding(\n",
    "                    input_dim=len(vocabs['WORDS']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    input_length=1)(input_word2)\n",
    "                embeddings_w2 = Flatten()(embeddings_w2)\n",
    "            \n",
    "        if featureset is not 'f1' and embedsl is not None:\n",
    "            # we must define also inputs and embeddings for lemmas\n",
    "            if len(vocabs['LEMMA']) != embedsl.shape[0]:\n",
    "                print(\"Lemma vocab size {v} must equal embeddings length {e}\".format(v=len(vocabs['LEMMA']), \n",
    "                                                                               e=embedsl.shape[0]))\n",
    "                return None\n",
    "            \n",
    "            # Pretrained Embedding layer for 2 lemmas\n",
    "            input_lemma1 = Input(shape=(1,), dtype='int32', name='lemma_input_1')\n",
    "            embeddings_l1 = Embedding(\n",
    "                input_dim=len(vocabs['LEMMA']), \n",
    "                output_dim=dim_embeddings, \n",
    "                weights=[embedsl], \n",
    "                input_length=1)(input_lemma1)\n",
    "            embeddings_l1 = Flatten()(embeddings_l1)\n",
    "                \n",
    "            # Embedding layer for second word\n",
    "            input_lemma2 = Input(shape=(1,), dtype='int32', name='lemma_input_2')\n",
    "            embeddings_l2 = Embedding(\n",
    "                input_dim=len(vocabs['LEMMA']), \n",
    "                output_dim=dim_embeddings, \n",
    "                weights=[embedsl], \n",
    "                input_length=1)(input_lemma2)\n",
    "            embeddings_l2 = Flatten()(embeddings_l2)\n",
    "            \n",
    "        elif featureset is not 'f1':\n",
    "            \n",
    "            # Embedding layer for 2 lemmas\n",
    "            input_lemma1 = Input(shape=(1,), dtype='int32', name='lemma_input_1')\n",
    "            embeddings_l1 = Embedding(\n",
    "                input_dim=len(vocabs['LEMMA']), \n",
    "                output_dim=dim_embeddings, \n",
    "                input_length=1)(input_lemma1)\n",
    "            embeddings_l1 = Flatten()(embeddings_l1)\n",
    "                \n",
    "            # Embedding layer for second word\n",
    "            input_lemma2 = Input(shape=(1,), dtype='int32', name='lemma_input_2')\n",
    "            embeddings_l2 = Embedding(\n",
    "                input_dim=len(vocabs['LEMMA']), \n",
    "                output_dim=dim_embeddings, \n",
    "                input_length=1)(input_lemma2)\n",
    "            embeddings_l2 = Flatten()(embeddings_l2)            \n",
    "            \n",
    "        \n",
    "        # define input for additional features\n",
    "        # note: dist is restricted to [-7 ... 7] so 15 values\n",
    "        if featureset == 'f1':\n",
    "            \"\"\" S.0.POS\n",
    "                B.0.POS\n",
    "                DIST\"\"\"\n",
    "            dim_features = len(vocabs['POS']) * 2 + 15\n",
    "        elif featureset == 'f2':\n",
    "            \"\"\" S.0.POS\n",
    "                S.0.LEMMA\n",
    "                S.0.MORPHO\n",
    "                S.-1.POS\n",
    "                B.0.POS\n",
    "                B.0.LEMMA\n",
    "                B.0.MORPHO\n",
    "                B.-1.POS\n",
    "                B.1.POS\n",
    "                DIST\"\"\"\n",
    "            dim_features = len(vocabs['POS']) * 4 + len(vocabs['LEMMA']) * 2 + len(vocabs['MORPHO']) * 2 + 15\n",
    "        else:\n",
    "            \"\"\" S.0.POS\n",
    "                S.0.LEMMA\n",
    "                S.0.MORPHO\n",
    "                S.-1.POS\n",
    "                B.0.POS\n",
    "                B.0.LEMMA\n",
    "                B.0.MORPHO\n",
    "                B.-2.POS\n",
    "                B.-1.POS\n",
    "                B.1.POS\n",
    "                DIST\"\"\"\n",
    "            # same size as 'f2'\n",
    "            dim_features = len(vocabs['POS']) * 5 + len(vocabs['LEMMA']) * 2 + len(vocabs['MORPHO']) * 2 + 15\n",
    "        print(\"  expecting {n} features dim\".format(n=dim_features))\n",
    "        \n",
    "        # define input for features\n",
    "        features_input = Input(shape=(dim_features,))\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            # concatenate features and embeddings\n",
    "            l = concatenate([embeddings_1, embeddings_2, features_input])\n",
    "        else:\n",
    "            l = features_input\n",
    "        \n",
    "        # adding dense layers\n",
    "        \n",
    "        l = Dense(128)(l)\n",
    "        l = Activation('relu')(l)\n",
    "        if dropout:\n",
    "            l = Dropout(0.15)(l)\n",
    "        l = Dense(128)(l)\n",
    "        l = Activation('relu')(l)\n",
    "        if dropout:\n",
    "            l = Dropout(0.15)(l)\n",
    "        \n",
    "        l = Dense(nb_classes)(l)\n",
    "        out = Activation('softmax')(l)\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            net_model = Model([input_word1, input_word2, features_input], out)\n",
    "        else:\n",
    "            net_model = Model(features_input, out)\n",
    "        \n",
    "        # not sure where to compile the model ...\n",
    "        net_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            \n",
    "        if net_model:\n",
    "            self.networks_[network_name] = net_model\n",
    "            # save initial state of this network (this also saves embedding layers, so we remove them)\n",
    "            save_network(network_name)\n",
    "            #del self.embeddings_for_words_\n",
    "            #del self.embeddings_for_lemmas_\n",
    "        \n",
    "        return net_model\n",
    "\n",
    "    def save_network(self, network_name):\n",
    "        if name in self.networks_:\n",
    "            self.networks_[network_name].save(os.path.join(self.path_, '/', network_name, '-net.h5'))\n",
    "        else:\n",
    "            print(\"Net \" + network_name + \" not found\")\n",
    "            \n",
    "    def load_network(self, network_name):\n",
    "        \"\"\"\n",
    "        Loads a keras network model (.h5, architecture and weights) from disk.\n",
    "        \"\"\"\n",
    "        self.networks_[network_name] = load_model(os.path.join(self.path_, network_name, '-net.h5'))\n",
    "        \n",
    "        return net_model            \n",
    "\n",
    "    def remove_network(self, network_name):\n",
    "        \"\"\"\n",
    "        Removes a network model from rnnmanager (not from disk !), if it exists it is returned by this method.\n",
    "        \"\"\"\n",
    "        if network_name in self.networks_.keys():\n",
    "            return self.networks_.pop(model_name, None)    \n",
    "    \n",
    "    def preprocess_embeddings(self, model_name, augment_vocabs=True):\n",
    "        \"\"\"\n",
    "        Preprocess embeddings for training a network for this model.\n",
    "        Notes:\n",
    "        - should be run once BEFORE creating a network, if you want to use pre-trained embeddings to train this network, \n",
    "          if not embeddings will be completely learned during training\n",
    "        - loaded embeddings should be deleted before loading/running a new network to avoid exhausting memory.\n",
    "        - if called again for another model embeddings will be REPLACED\n",
    "        - for already created network embedded remain and will be saved along with the network (no need to call this if\n",
    "          you load a saved network)\n",
    "            - at network creation, once net model is saved to disk with keras model.save(), loaded embeddings are deleted\n",
    "              to free memory\n",
    "            - to manually remove loaded embeddings, delete attributes embeddings_, embeddings_for_words_ and embeddings_for_lemmas_\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            Name of the TAL model.\n",
    "            \n",
    "        Returns: nothing\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        model = self.models_[model_name]\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        \n",
    "        if (self.embeddings_for_words_ is None and use_forms) or self.embeddings_for_lemmas_ is None:\n",
    "        \n",
    "            if self.embeddings_ is None:\n",
    "                print(\"preprocess_embeddings: loading original embeddings ...\")\n",
    "                self.embeddings_ = self.dm_.load_embeddings(lang)\n",
    "            \n",
    "            faligned = os.path.join(self.path_, 'cache', model_name + '-embeddings-forms-aligned.pkl')\n",
    "            if os.path.isfile(faligned):\n",
    "                self.embeddings_for_words_ = self.dm_.pickle_load(faligned)\n",
    "            if self.embeddings_for_words_ is None:\n",
    "                print(\"preprocess_embeddings: aligning embeddings for forms ...\")\n",
    "                self.embeddings_for_words_, words_not_found, words_matched = self.dm_.align_embeddings(\n",
    "                    vocabs['WORDS'], self.embeddings_, augment_vocab=augment_vocabs)\n",
    "                self.dm_.safe_pickle_dump(faligned, self.embeddings_for_words_)\n",
    "                    \n",
    "            # process vocab for lemmas even if not 'f1', so it can be reused for f2 or f3 for same language\n",
    "            faligned = os.path.join(self.path_, 'cache', model_name + '-embeddings-lemmas-aligned.pkl')\n",
    "            if os.path.isfile(faligned):\n",
    "                self.embeddings_for_lemmas_ = self.dm_.pickle_load(faligned)\n",
    "            if self.embeddings_for_lemmas_ is None:\n",
    "                print(\"preprocess_embeddings: aligning embeddings for lemmas ...\")\n",
    "                self.embeddings_for_lemmas_, words_not_found, words_matched = self.dm_.align_embeddings(\n",
    "                    vocabs['LEMMA'], self.embeddings_, augment_vocab=augment_vocabs)\n",
    "                self.dm_.safe_pickle_dump(faligned, self.embeddings_for_lemmas_)\n",
    "       \n",
    "        # free some memory - original embeddings should be useless now\n",
    "        del self.embeddings_\n",
    "        \n",
    "        return\n",
    "    \n",
    "            \n",
    "    def preprocess_data(self, model_name, X_train, y_train, X_dev, y_dev, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Preprocess data for training a network for this model.\n",
    "        - merges vocabs (WORDS) of X_dev into X_train\n",
    "        - merges vocabs (LEMMA) of X_dev into X_train, if any\n",
    "        - merges vocabs (MORPHO) of X_dev into X_train, if any\n",
    "        - merges vocabs (labels) of Y_dev and Y_test into Y_train\n",
    "        - converts to categorical (one-hot), features of X_* (POS, MORPHO, DIST)\n",
    "        \n",
    "        Note: arrays and vocabs are updated in place, so keep a copy of originals if required.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            TAL model to use for treating data (defines features and vocabs).\n",
    "            \n",
    "        X_train, y_train, X_dev, y_dev, X_test, y_test: arrays\n",
    "            Training, validation and test data to process.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        model = self.models_[model_name]\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model['use_forms']\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        vocabs_dev = model['vocabs_dev']\n",
    "        vocabs_test = model['vocabs_test']\n",
    "        \n",
    "        # Treat vocabularies\n",
    "        \n",
    "        if use_forms:\n",
    "            print(\"preprocess_data: merging WORDS vocabs from dev into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['WORDS'], vocab2=vocabs_dev['WORDS'], data=X_dev, columns=(0, 1))\n",
    "            # ... vocabs_dev['WORDS'] is now useless\n",
    "        \n",
    "        if 'POS' in vocabs:\n",
    "            nb_classes_pos = len(np.unique(vocabs['POS']))\n",
    "        if 'MORPHO' in vocabs:\n",
    "            print(\"preprocess_data: merging and aligning MORPHO vocabs from dev into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['MORPHO'], vocab2=vocabs_dev['MORPHO'], data=X_dev, columns=(4, 8))\n",
    "            # ... vocabs_dev['MORPHO'] is now useless\n",
    "            nb_classes_morpho = len(np.unique(vocabs['MORPHO']))\n",
    "        if 'LEMMA' in vocabs:\n",
    "            print(\"preprocess_data: merging and aligning LEMMA vocabs from dev into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['LEMMA'], vocab2=vocabs_dev['LEMMA'], data=X_dev, columns=(3, 7))\n",
    "            # ... vocabs_dev['LEMMA'] is now useless\n",
    "        \n",
    "        print(vocabs.keys())\n",
    "        print(vocabs_dev.keys())\n",
    "        print(\"preprocess_data: merging and aligning LABEL vocabs from dev into train ...\")\n",
    "        self.dm_.merge_vocabs(vocab1=vocabs['LABELS'], vocab2=vocabs_dev['LABELS'], data=y_dev, columns=(0,))\n",
    "        print(\"preprocess_data: merging and aligning LABEL vocabs from test into train ...\")\n",
    "        self.dm_.merge_vocabs(vocab1=vocabs['LABELS'], vocab2=vocabs_test['LABELS'], data=y_test, columns=(0,))\n",
    "        nb_classes = len(vocabs['LABELS'])\n",
    "            \n",
    "        # convert some features to one-hot encoding\n",
    "        \n",
    "        nb_classes_dist = 8\n",
    "        \n",
    "        \n",
    "        print(\"preprocess_data: converting {f} features to one-hot encoding...\".format(f=featureset))\n",
    "        \n",
    "        if featureset == 'f1':\n",
    "            cats_pos1 = to_categorical(X_train[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(X_train[:, 3], num_classes=nb_classes_pos)\n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_train[:, 4]), num_classes=nb_classes_dist)\n",
    "            X_train = np.column_stack((X_train[:, 0], X_train[:, 1], cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "            cats_pos1 = to_categorical(X_dev[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(X_dev[:, 3], num_classes=nb_classes_pos)\n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_dev[:, 4]), num_classes=nb_classes_dist)\n",
    "            X_dev = np.column_stack((X_dev[:, 0], X_dev[:, 1], cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "            cats_pos1 = to_categorical(X_test[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(X_test[:, 3], num_classes=nb_classes_pos)\n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_test[:, 4]), num_classes=nb_classes_dist)\n",
    "            X_test = np.column_stack((X_test[:, 0], X_test[:, 1], cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "        else: # 'f2' and 'f3' featuresets have same structure\n",
    "            \"\"\" \n",
    "            S.0.POS\n",
    "            S.0.LEMMA\n",
    "            S.0.MORPHO\n",
    "            S.-1.POS (f2, or S.1.POS for f3)\n",
    "            B.0.POS\n",
    "            B.0.LEMMA\n",
    "            B.0.MORPHO\n",
    "            B.-1.POS\n",
    "            B.1.POS\n",
    "            DIST\"\"\"                \n",
    "            cats_pos1    = to_categorical(X_train[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_train[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_train[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_train[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_train[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_train[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_train[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_train[:, 11]), num_classes=nb_classes_dist)\n",
    "            X_train = np.column_stack((X_train[:, 0], X_train[:, 1], cats_pos1, X_train[:, 3],\n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               X_train[:, 7], cat_trains_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "            \n",
    "            cats_pos1    = to_categorical(X_dev[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_dev[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_dev[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_dev[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_dev[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_dev[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_dev[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_dev[:, 11]), num_classes=nb_classes_dist)\n",
    "            X_dev = np.column_stack((X_dev[:, 0], X_dev[:, 1], cats_pos1, X_dev[:, 3],\n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               X_dev[:, 7], cat_trains_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "            \n",
    "\n",
    "            cats_pos1    = to_categorical(X_test[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_test[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_test[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_test[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_test[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_test[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_test[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_test[:, 11]), num_classes=nb_classes_dist)\n",
    "            X_test = np.column_stack((X_test[:, 0], X_test[:, 1], cats_pos1, X_test[:, 3],\n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               X_test[:, 7], cats_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "                        \n",
    "        print(\"preprocess_data: converting LABELs to one-hot encoding ...\")\n",
    "        y_train = to_categorical(y_train, num_classes=nb_classes)\n",
    "        y_dev = to_categorical(y_dev, num_classes=nb_classes)\n",
    "        y_test = to_categorical(y_test, num_classes=nb_classes)\n",
    "                \n",
    "        return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rnn\n",
    "\n",
    "# define a manager with hard limit of 100000 for vocabularies lengths\n",
    "rnn = RNNManager(path='.', max_vocab_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data\\f1_fr-train\n",
      "load_data: loaded X =  (679243, 6) , Y =  (679243,) , vocabs =  WORDS (42278), POS (20), LABELS (90), \n",
      "Loading data\\f1_fr-dev\n",
      "load_data: loaded X =  (68105, 6) , Y =  (68105,) , vocabs =  WORDS (9275), POS (20), LABELS (74), \n",
      "Loading data\\f1_fr-test\n",
      "load_data: loaded X =  (19119, 6) , Y =  (19119,) , vocabs =  WORDS (3285), POS (20), LABELS (68), \n",
      "loaded data in  274.11958408355713\n",
      "pickle.dump .\\f1_fr-model.pkl\n",
      "\n",
      "dict_keys(['WORDS', 'POS', 'LABELS'])\n",
      "dict_keys(['WORDS', 'POS', 'LABELS'])\n",
      "preprocess_data: merging and aligning LABEL vocabs from dev into train ...\n",
      "preprocess_data: merging and aligning LABEL vocabs from test into train ...\n",
      "preprocess_data: converting f1 features to one-hot encoding...\n",
      "preprocess_data: converting LABELs to one-hot encoding ...\n",
      "Preprocessed data in  2.1951284408569336\n",
      "\n",
      "preprocess_embeddings: loading original embeddings ...\n",
      "pickle.load C:\\IAAA\\tal-github\\cache\\wiki.fr.vec.pkl\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "print()\n",
    "\n",
    "dm = DataManager()\n",
    "X_train, y_train, vocabs_train = dm.load_data()\n",
    "X_dev, y_dev, vocabs_dev = dm.load_data(phase='dev')\n",
    "X_test, y_test, vocabs_test = dm.load_data(phase='test')\n",
    "\n",
    "print(\"loaded data in \", time.time() - t)\n",
    "\n",
    "nb_classes = len(vocabs_train['LABELS'])\n",
    "\n",
    "lang = 'fr'\n",
    "featureset = 'f1'\n",
    "forms = False\n",
    "model_name = \"{featureset}_{lang}\".format(featureset=featureset, lang=lang)\n",
    "\n",
    "\n",
    "\n",
    "# create a TAL model for this combination, using forms\n",
    "rnn.remove_model(model_name)\n",
    "rnn.create_model(model_name, lang, featureset, forms, vocabs_train, vocabs_dev, vocabs_test)\n",
    "rnn.save_model(model_name)\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "print()\n",
    "# preprocess X/Y data\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = rnn.preprocess_data(model_name, X_train, y_train, X_dev, y_dev, X_test, y_test)\n",
    "\n",
    "print(\"Preprocessed data in \", time.time() - t)\n",
    "\n",
    "t = time.time()\n",
    "print()\n",
    "# load pretrained embeddings\n",
    "rnn.preprocess_embeddings(model_name, augment_vocabs=True)\n",
    "\n",
    "print(\"Preprocessed embeddings in \", time.time() - t)\n",
    "\n",
    "t = time.time()\n",
    "print()\n",
    "# create a classifier for this TAL model\n",
    "network_name = \"{model}\".format(model=model_name)\n",
    "net = rnn.create_network(network_name, model_name, nb_classes=nb_classes, dropout=True)\n",
    "\n",
    "print(\"Created net in \", time.time() - t)\n",
    "\n",
    "print(rnn.networks_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "446113/446113 [==============================] - 661s 1ms/step - loss: 0.6085 - acc: 0.7874\n",
      "Epoch 2/10\n",
      "446113/446113 [==============================] - 658s 1ms/step - loss: 0.5180 - acc: 0.8190\n",
      "Epoch 3/10\n",
      "446113/446113 [==============================] - 659s 1ms/step - loss: 0.4499 - acc: 0.8433\n",
      "Epoch 4/10\n",
      "446113/446113 [==============================] - 659s 1ms/step - loss: 0.4023 - acc: 0.8598\n",
      "Epoch 5/10\n",
      "446113/446113 [==============================] - 657s 1ms/step - loss: 0.3674 - acc: 0.8709\n",
      "Epoch 6/10\n",
      "446113/446113 [==============================] - 658s 1ms/step - loss: 0.3413 - acc: 0.8792\n",
      "Epoch 7/10\n",
      "446113/446113 [==============================] - 660s 1ms/step - loss: 0.3205 - acc: 0.8865\n",
      "Epoch 8/10\n",
      "446113/446113 [==============================] - 662s 1ms/step - loss: 0.3065 - acc: 0.8905 0s - loss: 0.3064 - acc:\n",
      "Epoch 9/10\n",
      "446113/446113 [==============================] - 659s 1ms/step - loss: 0.2932 - acc: 0.8960\n",
      "Epoch 10/10\n",
      "446113/446113 [==============================] - 657s 1ms/step - loss: 0.2829 - acc: 0.8994\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display:none\">\n",
       "                <audio onended=\"this.parentNode.removeChild(this)\" controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"http://www.soundjay.com/button/beep-07.wav\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              </div>"
      ],
      "text/plain": [
       "<__main__.InvisibleAudio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = net.fit([X_train[:, 0], X_train[:, 1], X_train[:, 2:X_train.shape[1]]], \n",
    "                  y_train,\n",
    "                  validation_data=(\n",
    "                      [X_dev[:, 0], X_dev[:, 1], X_dev[:, 2:X_dev.shape[1]]], \n",
    "                      y_dev)\n",
    "                  epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.save_network(network_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- il n'y a pas les mots dans les features (seulement les lemmes)\n",
    "\n",
    "- ajouter les mots uniquement pour l'expérience en plus\n",
    "\n",
    "- embeddings pour les lemmes: il faudrait ajouter tous les mots des embeddings ! (et pas les enlever lors de l'alignement)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
