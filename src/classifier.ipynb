{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Licenses\n",
    "GloVe\n",
    "Public Domain Dedication and License v1.0 whose full text can be found at: http://www.opendatacommons.org/licenses/pddl/1.0/\n",
    "\n",
    "Facebookresearch / FastText words embeddings\n",
    "https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "@article{bojanowski2016enriching,\n",
    "  title={Enriching Word Vectors with Subword Information},\n",
    "  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n",
    "  journal={arXiv preprint arXiv:1607.04606},\n",
    "  year={2016}\n",
    "}\n",
    "\n",
    "License Creative Commons Attribution-Share-Alike License 3.0 (https://creativecommons.org/licenses/by-sa/3.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class DataManager:\n",
    "    \n",
    "    root_dir_ = '.'\n",
    "    \n",
    "    UNKNOWN_WORD = \"<UNK>\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, root_dir='.', unknown_word='<UNK>'):\n",
    "        self.root_dir_ = root_dir\n",
    "        self.UNKNOWN_WORD = unknown_word\n",
    "        \n",
    "    def load_dummy_data(self):\n",
    "        \"\"\"\n",
    "        This method makes available some dummy training data.\n",
    "        \"\"\"\n",
    "        X = self.pickle_load(os.path.join('data', '/fr_X.pkl'))\n",
    "        Y = self.pickle_load(\"data/fr_Y.pkl\")\n",
    "        vocab_mots = self.pickle_load(\"data/fr_vocab_mots.pkl\")\n",
    "        vocab_pdd = self.pickle_load(\"data/fr_vocab_pdd.pkl\")\n",
    "        vocab_liaisons = self.pickle_load(\"data/fr_vocab_liaisons.pkl\")\n",
    "        return X, Y, vocab_mots, vocab_pdd, vocab_liaisons\n",
    "        \n",
    "    def load_dummy_data_2(self):\n",
    "        \"\"\"\n",
    "        This method makes available some dummy training data.\n",
    "        \"\"\"\n",
    "        data = self.pickle_load(\"data/f1_fr_project_ok_bool.pkl\")\n",
    "        return data['X'], data['Y'], data['vocab_mots'], data['vocab_pdd'], data['vocab_liaisons'] \n",
    "    \n",
    "    def load_data(self, phase='train', lang='fr', featureset='f1'):\n",
    "        \"\"\"\n",
    "        Loads a dataset for a specific lang and feature set, and phase (train/dev/test).\n",
    "        Note: if unknown word is not present in WORDS or LEMMA vocab, then it is appended.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        phase: str\n",
    "            'train', 'dev' or 'test'\n",
    "            \n",
    "        lang: str\n",
    "        \n",
    "        featureset: str\n",
    "            'f1', 'f2' or 'f3'\n",
    "        \n",
    "        \"\"\"\n",
    "        name = \"{featureset}_{lang}-{phase}\".format(lang=lang, featureset=featureset, phase=phase)\n",
    "        fname = os.path.join(self.root_dir_, 'data', name)\n",
    "        data = self.pickle_load(fname)\n",
    "        if data:\n",
    "            vocabs = {}\n",
    "            X = np.array(data['X'])\n",
    "            Y = np.array(data['Y'])\n",
    "            vocabs['WORDS'] = data['vocab_mots']\n",
    "            vocabs['POS'] = data['vocab_pdd']\n",
    "            vocabs['LABELS'] = data['vocab_liaisons']\n",
    "            if isinstance(vocabs['WORDS'], np.ndarray):\n",
    "                vocabs['WORDS'] = vocabs['WORDS'].tolist()\n",
    "            if self.UNKNOWN_WORD not in vocabs['WORDS']:\n",
    "                vocabs['WORDS'].append(self.UNKNOWN_WORD)\n",
    "                \n",
    "            if isinstance(vocabs['POS'], np.ndarray):\n",
    "                vocabs['POS'] = vocabs['POS'].tolist()\n",
    "            if isinstance(vocabs['LABELS'], np.ndarray):\n",
    "                vocabs['LABELS'] = vocabs['LABELS'].tolist()\n",
    "            if self.UNKNOWN_WORD not in vocabs['LABELS']:\n",
    "                vocabs['LABELS'].append(self.UNKNOWN_WORD)\n",
    "                \n",
    "            if featureset == 'f2' or featureset == 'f3':\n",
    "                vocabs['MORPHO'] = data['vocab_morpho']\n",
    "                vocabs['LEMMA'] = data['vocab_lemma']\n",
    "                if isinstance(vocabs['MORPHO'], np.ndarray):\n",
    "                    vocabs['MORPHO'] = vocabs['MORPHO'].tolist()\n",
    "                if self.UNKNOWN_WORD not in vocabs['MORPHO']:\n",
    "                    vocabs['MORPHO'].append(self.UNKNOWN_WORD)\n",
    "                if isinstance(vocabs['LEMMA'], np.ndarray):\n",
    "                    vocabs['LEMMA'] = vocabs['LEMMA'].tolist()\n",
    "                if self.UNKNOWN_WORD not in vocabs['LEMMA']:\n",
    "                    vocabs['LEMMA'].append(self.UNKNOWN_WORD)\n",
    "                          \n",
    "            print(\"load_data: loaded X = \", X.shape, \", Y = \", Y.shape, \", vocabs = \", \n",
    "                  (''.join(\"{key} ({len}), \".format(key=k, len=len(vocabs[k])) for k in vocabs.keys())))\n",
    "            return X, Y, vocabs\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def merge_vocabs(self, vocab1, vocab2, data, columns, test_mode=False, verbose=False):\n",
    "        \"\"\"\n",
    "        Merges vocab2 into vocab1, updating accordingly words references in data columns.\n",
    "        For all words in vocab2 with index idx_vocab2:\n",
    "        - if it exists in vocab1, then words features of data in columns refering to idx_vocab2 will be replaced by idx_vocab1\n",
    "        - if it does not exist in vocab1, \n",
    "          - If test_mode is False, then it will be appended to vocab1 then replacement will be done as above in data\n",
    "          - If test_mode is True, then pointers to idx_vocab2 in data columns, will be replaced by pointers to unknown word\n",
    "            from vocab1 (this may be used to align a test vocab/data to a neural network vocab, as we are not supposed\n",
    "            to change then neural network vocab once trained on train/dev data)\n",
    "        When test_mode is True, vocab1 is left unchanged.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        vocab1: list(str)\n",
    "            A list of words from original vocabulary.\n",
    "                        \n",
    "        vocab2: list(str)\n",
    "            A list of words from vocabulary to be merged into vocab1 (used to build X2).\n",
    "            \n",
    "        data: array\n",
    "            Array containing the words indices to be updated by merge of vocabularies.\n",
    "            Usually this should be an array with samples as first dimension (rows), then columns for features.\n",
    "\n",
    "        columns: tuple(int)\n",
    "            Indices of columns to be updated in data.\n",
    "            \n",
    "        test_mode: boolean\n",
    "            \n",
    "        Returns:\n",
    "        \n",
    "        vocab1: list(str)\n",
    "            A new vocabulary with missing words appended to vocab1.\n",
    "            Note: vocab1 is also update in place, meaning this function modified original vocab1.\n",
    "        \n",
    "        ! data is updated in place.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        vocab1_ = vocab1.copy()\n",
    "        data_ = data.copy()\n",
    "        \n",
    "        unknown_idx = -1\n",
    "        if self.UNKNOWN_WORD in vocab1:\n",
    "            unknown_idx = vocab1.index(self.UNKNOWN_WORD)\n",
    "        \n",
    "        for idx_vocab2, w in enumerate(vocab2):\n",
    "            # treat vocabs\n",
    "            if w in vocab1:\n",
    "                if verbose:\n",
    "                    print(\"word [{i}]={w} found in vocab1 at {j}\".format(i=idx_vocab2, w=w, j=vocab1.index(w)))\n",
    "                idx_vocab1 = vocab1.index(w)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"word [{i}]={w} not found in vocab1\".format(i=idx_vocab2, w=w))\n",
    "                if not test_mode:\n",
    "                    vocab1.append(w)\n",
    "                    idx_vocab1 = len(vocab1) - 1\n",
    "                elif unknown_idx is not -1:\n",
    "                    # if in test mode, we associate missing word to \"unknown word\"\n",
    "                    idx_vocab1 = unknown_idx\n",
    "                else:\n",
    "                    print(\"merge_vocabs: word [{i}]={w} not found in vocab1 but no unknown word '{unk}' defined in vocab1\"\n",
    "                         .format(i=idx_vocab2, w=w, unk=self.UNKNOWN_WORD))\n",
    "                \n",
    "            # replace all references in data\n",
    "            for i in range(len(data_)):\n",
    "                if len(data_.shape) == 1:\n",
    "                    if data_[i] == idx_vocab2:\n",
    "                        if verbose:\n",
    "                            print(\"Replacing word [{i}]= {idx}, {w} index {fro} to {to}\"\n",
    "                                    .format(i=i, idx=idx_vocab2, w=vocab2[idx_vocab2], \n",
    "                                            fro=idx_vocab2, to=idx_vocab1))\n",
    "                        data[i] = idx_vocab1\n",
    "                        #Replacing word [796]= 1, LEFT_advcl index 1 to 2\n",
    "                        \n",
    "                else:\n",
    "                    for idx, j in enumerate(columns):\n",
    "                        if data_[i][j] == idx_vocab2:\n",
    "                            if verbose:\n",
    "                                print(\"Replacing word [{i},{j}]= {idx}, {w} index {fro} to {to}\"\n",
    "                                      .format(i=i, j=j, idx=idx_vocab2, w=vocab2[idx_vocab2], \n",
    "                                              fro=idx_vocab2, to=idx_vocab1))\n",
    "                            data[i][j] = idx_vocab1\n",
    "\n",
    "        return vocab1, data\n",
    "        \n",
    "\n",
    "    # some utilities for saving results\n",
    "    def safe_pickle_dump(self, filename, obj):\n",
    "        \"\"\"\n",
    "        Serializes an object to file, creating directory structure if it does not exist.\n",
    "        \"\"\"\n",
    "        name = filename\n",
    "        print(\"pickle.dump \"+name)\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(name), exist_ok=True)\n",
    "            f = open(name,\"wb\")\n",
    "            pickle.dump(obj,f)\n",
    "            f.close()\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "        return True\n",
    "    \n",
    "    def pickle_load(self, filename):\n",
    "        \"\"\"\n",
    "        Deserialize an object from a file created with pickle.dump.\n",
    "        Returns False if this failed.\n",
    "        \"\"\"\n",
    "        name = filename\n",
    "        print(\"pickle.load \"+name)\n",
    "        try:\n",
    "            f = open(name,\"rb\")\n",
    "            obj = pickle.load(f)\n",
    "            f.close()\n",
    "            return obj\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "      \n",
    "        return None\n",
    "        \n",
    "    def load_embeddings(self, lang, type='fasttext'):\n",
    "        \"\"\"\n",
    "        Loads an embeddings file depending on its type and language.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        type: str\n",
    "            Only \"fasttext\" is supported.\n",
    "            \n",
    "        lang: str\n",
    "            See load_fasttext_embeddings(lang)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.load_fasttext_embeddings(lang)\n",
    "        \n",
    "    def load_fasttext_embeddings(self, lang):\n",
    "        \"\"\"\n",
    "        Loads a fasttext embedding, chosen depending on lang parameter provided.\n",
    "        File expected as root_dir_/data/embeddings/facebookresearch/wiki.{lang}.vec\n",
    "        (or as root_dir_/cache/wiki.{lang}.vec.pkl if already loaded once)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        lang: str\n",
    "            One of 'fr', 'ja', 'en', 'nl' (or additional ones depending on embeddings present on disk).\n",
    "        \n",
    "        \"\"\"\n",
    "        data_dict = {}\n",
    "        apax = \"\"\n",
    "        \n",
    "        pickle_fname = \"wiki.{lang}.vec.pkl\".format(lang=lang)\n",
    "        pickle_ffname = os.path.join(self.root_dir_, 'cache', pickle_fname)\n",
    "             \n",
    "        if os.path.isfile(pickle_ffname):\n",
    "            data_dict = self.pickle_load(pickle_ffname)\n",
    "            print(\"Embedding for {lang} loaded from {fname}\".format(lang=lang, fname=pickle_ffname))\n",
    "        else: \n",
    "            fname = \"wiki.{lang}.vec\".format(lang=lang)\n",
    "            data_file = os.path.join(self.root_dir_, 'data', 'embeddings', 'facebookresearch', fname)\n",
    "        \n",
    "            fin = io.open(data_file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "            n, d = map(int, fin.readline().split())\n",
    "            \n",
    "            for line in fin:\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                data_dict[tokens[0]] = list(map(float, tokens[1:]))\n",
    "            print(\"Embedding for {lang} loaded from {fname}\".format(lang=lang, fname=data_file))\n",
    "            # save embeddings as array format to improve speed next time\n",
    "            self.safe_pickle_dump(pickle_ffname, data_dict)\n",
    "        \n",
    "        # adds unknown word if required, using embedding of apax word\n",
    "        if self.UNKNOWN_WORD not in data_dict.keys():\n",
    "            apax = list(data_dict.keys())[-1] # python does not guarantee order of dict keys but normally should be ok\n",
    "            print(\"debug apax \", apax)\n",
    "            data_dict[self.UNKNOWN_WORD] = data_dict[apax]\n",
    "\n",
    "        print(\"load_fasttext_embeddings: loaded \", len(data_dict), \" words vectors, apax '\", apax, \"'\")\n",
    "        return data_dict, apax\n",
    "    \n",
    "    def get_words_to_match_for_embeddings(self, word):\n",
    "        \"\"\"\n",
    "        Returns a list with same word, word lowercased, word lowercased and dashes removed, \n",
    "        then all this plus \\xa0 removed.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        word: str\n",
    "            Word to be transformed.\n",
    "            \n",
    "        Returns: list\n",
    "            List of transformed word forms.\n",
    "        \"\"\"\n",
    "        return [word, word.lower(), word.lower().replace('-', ''), word.lower().replace('-', '').replace('\\\\xa0', ' ')]\n",
    "    \n",
    "    def align_embeddings(self, vocab, embeddings, augment_vocab=True, max_vocab_size=-1):\n",
    "        \"\"\"\n",
    "        Generates aligned embeddings from original embeddings, and a vocabulary of words.\n",
    "        Words from vocabulary may not exist in original embeddings, in this case a random vector is generated.\n",
    "        Words matching is done as (by priority) : exact match, then case insensitively, then with dash ('-') removed.\n",
    "        Exception: unknown word is matched with lowest frequency word from embeddings corpus (with fasttext, it is the last\n",
    "        one from the list).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        vocab: array\n",
    "            An array containing each word in the vocabulary.\n",
    "            \n",
    "        embeddings: dict\n",
    "            A dict object with words as keys and their embeddings (as a vector array) as values.\n",
    "            \n",
    "        augment_vocab: boolean\n",
    "            If True, then all words from embeddings not existing in vocab, are appended to vocab (up to max_vocab_size).\n",
    "            \n",
    "        max_vocab_size: int\n",
    "            Maximum length of resulting vocab if augment_vocab is True and if max_vocab_size < original vocab length.\n",
    "            If -1 then there is no limit.\n",
    "            Note: resulting vocab size can't be < original vocab size, whatever the value of max_vocab_size.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        aligned_embeddings: list\n",
    "            An array containing:\n",
    "          - for words from vocab found in embeddings, the corresponding embedding at same index as in vocab.\n",
    "          - for words not found in embeddings, a random vector, at same index as in vocab.\n",
    "          - if augment_vocab is True, all remaining words from embeddings (not found in vocab) are added after\n",
    "            len(vocab)\n",
    "        \n",
    "        words_not_found: list\n",
    "            An array containing indices (based on vocab) of words not found in embeddings and replaced by random\n",
    "            values.\n",
    "            \n",
    "        words_matched: list\n",
    "            An array of strings of words based on vocab, as they were matched in embeddings.\n",
    "            For example if lowercased word from vocab was matched, then lowercase version of this word will be found\n",
    "            in this table (whereas the original case sensitive word will remain as is in vocab array)\n",
    "        \n",
    "        \"\"\"\n",
    "        dim_embedding = len(embeddings[list(embeddings.keys())[0]]) # find length from value of 'first' key\n",
    "        \n",
    "        print(\"align_embeddings: aligning embeddings ({elen},{edim}) with vocab ({vlen}) using at most {max}\"\n",
    "             .format(elen=len(embeddings), edim=dim_embedding, vlen=len(vocab), max=max_vocab_size))\n",
    "        \n",
    "        cur_size = len(vocab) # to avoid computing vocab len at each iteration\n",
    "        \n",
    "        # first append missing embeddings to vocab, if required, to limit unknown words\n",
    "        if augment_vocab:\n",
    "            for embedding_word in embeddings.keys():\n",
    "                # adds missing word only up to max vocab size, if used\n",
    "                if max_vocab_size is not -1 and cur_size > max_vocab_size:\n",
    "                    break\n",
    "                elif embedding_word not in vocab:\n",
    "                    vocab.append(embedding_word)\n",
    "                    cur_size = cur_size + 1\n",
    "#                    if cur_size % 100 == 0:\n",
    "#                        print(\"debug embed cur_size \", cur_size)\n",
    "        \n",
    "            print(\"align_embeddings: new augmented vocab size : \", len(vocab))\n",
    "        \n",
    "        aligned_embeddings = np.zeros((len(vocab), dim_embedding))\n",
    "        words_not_found = []\n",
    "        words_matched = [None] * len(vocab)\n",
    "        for idx_mot, mot in enumerate(vocab):\n",
    "            words_to_match = self.get_words_to_match_for_embeddings(mot)\n",
    "            for word_to_match in words_to_match:\n",
    "                if word_to_match in embeddings.keys():\n",
    "                    aligned_embeddings[idx_mot] = embeddings[word_to_match]\n",
    "                    words_matched[idx_mot] = word_to_match\n",
    "                    break\n",
    "            if words_matched[idx_mot] is None:\n",
    "                words_not_found.append(idx_mot)\n",
    "                words_matched[idx_mot] = mot\n",
    "                aligned_embeddings[idx_mot] = np.random.rand(dim_embedding)\n",
    "        \n",
    "        print(\"align_embeddings: new embeddings shape {shap}, words not found {wnf}, words found {wf}\"\n",
    "             .format(shap=aligned_embeddings.shape, wnf=len(words_not_found), wf=len(words_matched)))\n",
    "        return aligned_embeddings, words_not_found, words_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation, Concatenate, Embedding, concatenate, Flatten, Dropout\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DependencyClassifier:\n",
    "    \n",
    "    UNKNOWN_WORD = '<UNK>' \n",
    "    \n",
    "    MAX_VOCAB_SIZE = -1\n",
    "    \n",
    "    models_ = {}\n",
    "    networks_ = {}\n",
    "    path_ = '.'\n",
    "    \n",
    "    current_model_ = None\n",
    "    current_network_ = None\n",
    "    \n",
    "    embeddings_ = None\n",
    "    embeddings_for_words_ = None\n",
    "    embeddings_for_lemmas_ = None\n",
    "    \n",
    "    dm_ = DataManager()\n",
    "    \n",
    "    def __init__(self, path='.', max_vocab_size=-1, unknown_word='<UNK>'):\n",
    "        \"\"\"\n",
    "        Class to handle neural networks for TAL - TBP AE purpose.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        path: str\n",
    "            Root path to find files (path/.), scripts (path/.), cache (path/cache), embeddings (path/data/embeddings/...) ...\n",
    "            Current path by default.\n",
    "            \n",
    "        max_vocab_size: int\n",
    "            Maximum length of a vocabulary - hence for an embedding matrix used in a network.\n",
    "            This can be set to limit amount of memory used by vocabs and embeddings.\n",
    "            In practice, original pre-trained embeddings vectors are truncated up to this length, considering\n",
    "            that embeddings used (fasttext) are ordered from most frequent to least frequent word.\n",
    "            -1 (default value) means no limitation.\n",
    "        \n",
    "        \"\"\"\n",
    "        if path != '.':\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.path_ = path\n",
    "        self.MAX_VOCAB_SIZE = max_vocab_size\n",
    "        self.UNKNOWN_WORD = '<UNK>'\n",
    "        self.dm_ = DataManager(self.path_)\n",
    "        \n",
    "    def get_model(self, model_name):\n",
    "        \"\"\"\n",
    "        Returns an existing TAL model, or None.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            Name of the model.\n",
    "        \"\"\"\n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        return self.models_[model_name]\n",
    "    \n",
    "    def get_current_model(self):\n",
    "        return self.models_[self.current_model_]\n",
    "\n",
    "    def create_model(self, model_name, lang, featureset, use_forms, vocabs, vocabs_dev, vocabs_test, \n",
    "                     status=False, test_status=False, embeddings_file=None):\n",
    "        \"\"\"\n",
    "        Creates a new TAL model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            Name of this model.\n",
    "            \n",
    "        lang: str\n",
    "            'fr', 'ja', 'nl', 'en'\n",
    "            \n",
    "        featureset: str\n",
    "            'f1', 'f2' or 'f3'\n",
    "            \n",
    "        use_forms: boolean\n",
    "            If true then both words forms are added on top of the features.\n",
    "            \n",
    "        vocabs, vocabs_dev, vocabs_test: dict\n",
    "            Vocabs for learning task, with keys 'WORDS', 'POS', 'MORPHO' and/or 'LEMMA' (or 'LABELS' for targets).\n",
    "        \n",
    "        status: boolean\n",
    "            If a network was prepared, created and trained for this model.\n",
    "            \n",
    "        test_status: boolean\n",
    "            If test results were produced for this model (ie conllu test file)\n",
    "        \n",
    "        \"\"\"\n",
    "        model = {'name': model_name,\n",
    "                'lang': lang,\n",
    "                'featureset': featureset,\n",
    "                'use_forms': use_forms,\n",
    "                'vocabs': vocabs,\n",
    "                'vocabs_dev': vocabs_dev,\n",
    "                'vocabs_test': vocabs_test,\n",
    "                'status': status,\n",
    "                'test_status': test_status,\n",
    "                'embeddings_file': embeddings_file}\n",
    "        print(\"featureset \", featureset)\n",
    "        #print(\"create model \", model)\n",
    "        if model_name not in self.models_.keys():\n",
    "            self.models_[model_name] = model\n",
    "        \n",
    "        self.current_model_ = model_name\n",
    "        \n",
    "        return model\n",
    "            \n",
    "    def remove_model(self, model_name):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if model_name in self.models_.keys():\n",
    "            if self.current_model_ == model_name:\n",
    "                self.current_model_ = None\n",
    "            return self.models_.pop(model_name, None)\n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if model_name in self.models_:\n",
    "            self.dm_.safe_pickle_dump(os.path.join(self.path_, model_name + '-model.pkl'), self.models_[model_name])\n",
    "        else:\n",
    "            print(\"Model \" + model_name + \" not found\")\n",
    "            \n",
    "    def load_model(self, model_name):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        model_conf = self.dm_.pickle_load(os.path.join(self.path_, model_name + '-model.pkl'))\n",
    "        if model_conf is not None:\n",
    "            self.create_model(model_name=model_conf['name'], lang=model_conf['lang'], featureset=model_conf['featureset'], \n",
    "                              vocabs=model_conf['vocabs'], vocabs_dev=model_conf['vocabs_dev'],\n",
    "                              vocabs_test=model_conf['vocabs_test'], use_forms=model_conf['use_forms'], \n",
    "                              status=model_conf['status'], test_status=model_conf['test_status'])\n",
    "            self.current_model_ = model_name\n",
    "            return True\n",
    "        return False\n",
    "            \n",
    "    def set_model_done(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            self.models_[model_name]['status'] = True\n",
    "        \n",
    "    def set_model_undone(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            self.models_[model_name]['status'] = True\n",
    "            \n",
    "    def get_model_status(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            return self.models_[model_name]['status']\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def set_model_test_done(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            self.models_[model_name]['test_status'] = True\n",
    "        \n",
    "    def set_model_test_undone(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            self.models_[model_name]['test_status'] = True\n",
    "            \n",
    "    def get_model_test_status(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            return self.models_[model_name]['test_status']\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def create_network(self, network_name, model_name, nb_classes, dropout=False, hidden_dim=200):\n",
    "        \"\"\"\n",
    "        Creates a keras architecture appropriate for a TAL model.\n",
    "        Note: a number of parameters comes from the TAL model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        nb_classes: int\n",
    "            Number of classes recognized by the network.\n",
    "            \n",
    "        dropout: boolean\n",
    "            Whether to add dropout layers or not.\n",
    "            \n",
    "        hidden_dim: int\n",
    "            The size of the hidden layers.\n",
    "            (network will consist of a first layer of hidden_dim and a second layer of hidden_dim/2 neurons)\n",
    "            \n",
    "        \"\"\"\n",
    "        net_model = None\n",
    "        \n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        \n",
    "        model = self.models_[model_name]\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model['use_forms']\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        \n",
    "        net_model = Model()\n",
    "                       \n",
    "        #if embeddings != None:\n",
    "            \n",
    "        embedsw = self.embeddings_for_words_\n",
    "        embedsl = self.embeddings_for_lemmas_\n",
    "        \n",
    "        if embedsw is not None:\n",
    "            dim_embeddings = embedsw.shape[1]\n",
    "        else:\n",
    "            dim_embeddings = 300\n",
    "        \n",
    "        concat_layers = []\n",
    "        input_layers = []\n",
    "        \n",
    "        if use_forms:\n",
    "            if embedsw is not None:\n",
    "            \n",
    "                if len(vocabs['WORDS']) != embedsw.shape[0]:\n",
    "                    print(\"Words vocab size {v} must equal embeddings length {e}\".format(v=len(vocabs['WORDS']), \n",
    "                                                                               e=embedsw.shape[0]))\n",
    "                    return None\n",
    "            \n",
    "                # Pretrained Embedding layer for 2 words\n",
    "                input_word1 = Input(shape=(1,), dtype='int32', name='word_input_1')\n",
    "                embeddings_w1 = Embedding(\n",
    "                    input_dim=len(vocabs['WORDS']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    weights=[embedsw], \n",
    "                    input_length=1)(input_word1)\n",
    "                embeddings_w1 = Flatten()(embeddings_w1)\n",
    "                \n",
    "                # Embedding layer for second word\n",
    "                input_word2 = Input(shape=(1,), dtype='int32', name='word_input_2')\n",
    "                embeddings_w2 = Embedding(input_dim=len(vocabs['WORDS']), \n",
    "                                     output_dim=dim_embeddings, \n",
    "                                     weights=[embedsw], \n",
    "                                     input_length=1)(input_word2)\n",
    "                #embeddings_2 = embeddings_layer(input_word2) # sharing weights between both words embeddings\n",
    "                embeddings_w2 = Flatten()(embeddings_w2)\n",
    "            \n",
    "            else:\n",
    "                # Embedding layer for 2 words\n",
    "                input_word1 = Input(shape=(1,), dtype='int32', name='word_input_1')\n",
    "                embeddings_w1 = Embedding(\n",
    "                    input_dim=len(vocabs['WORDS']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    input_length=1)(input_word1)\n",
    "                embeddings_w1 = Flatten()(embeddings_w1)\n",
    "                \n",
    "                # Embedding layer for second word\n",
    "                input_word2 = Input(shape=(1,), dtype='int32', name='word_input_2')\n",
    "                embeddings_w2 = Embedding(\n",
    "                    input_dim=len(vocabs['WORDS']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    input_length=1)(input_word2)\n",
    "                embeddings_w2 = Flatten()(embeddings_w2)\n",
    "                \n",
    "            concat_layers.append(embeddings_w1)\n",
    "            concat_layers.append(embeddings_w2)\n",
    "            input_layers.append(input_word1)\n",
    "            input_layers.append(input_word2)\n",
    "            \n",
    "        if 'LEMMA' in vocabs.keys():\n",
    "            if featureset is not 'f1' and embedsl is not None:\n",
    "                # we must define also inputs and embeddings for lemmas\n",
    "                if len(vocabs['LEMMA']) != embedsl.shape[0]:\n",
    "                    print(\"Lemma vocab size {v} must equal embeddings length {e}\".format(v=len(vocabs['LEMMA']), \n",
    "                                                                                   e=embedsl.shape[0]))\n",
    "                    return None\n",
    "            \n",
    "                # Pretrained Embedding layer for 2 lemmas\n",
    "                input_lemma1 = Input(shape=(1,), dtype='int32', name='lemma_input_1')\n",
    "                embeddings_l1 = Embedding(\n",
    "                    input_dim=len(vocabs['LEMMA']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    weights=[embedsl], \n",
    "                    input_length=1)(input_lemma1)\n",
    "                embeddings_l1 = Flatten()(embeddings_l1)\n",
    "                \n",
    "                # Embedding layer for second word\n",
    "                input_lemma2 = Input(shape=(1,), dtype='int32', name='lemma_input_2')\n",
    "                embeddings_l2 = Embedding(\n",
    "                    input_dim=len(vocabs['LEMMA']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    weights=[embedsl], \n",
    "                    input_length=1)(input_lemma2)\n",
    "                embeddings_l2 = Flatten()(embeddings_l2)\n",
    "            \n",
    "                concat_layers.append(embeddings_l1)\n",
    "                concat_layers.append(embeddings_l2)\n",
    "                input_layers.append(input_lemma1)\n",
    "                input_layers.append(input_lemma2)\n",
    "            \n",
    "            elif featureset is not 'f1':\n",
    "            \n",
    "                # Embedding layer for 2 lemmas\n",
    "                input_lemma1 = Input(shape=(1,), dtype='int32', name='lemma_input_1')\n",
    "                embeddings_l1 = Embedding(\n",
    "                    input_dim=len(vocabs['LEMMA']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    input_length=1)(input_lemma1)\n",
    "                embeddings_l1 = Flatten()(embeddings_l1)\n",
    "                    \n",
    "                # Embedding layer for second word\n",
    "                input_lemma2 = Input(shape=(1,), dtype='int32', name='lemma_input_2')\n",
    "                embeddings_l2 = Embedding(\n",
    "                    input_dim=len(vocabs['LEMMA']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    input_length=1)(input_lemma2)\n",
    "                embeddings_l2 = Flatten()(embeddings_l2)  \n",
    "            \n",
    "                concat_layers.append(embeddings_l1)\n",
    "                concat_layers.append(embeddings_l2)\n",
    "                input_layers.append(input_lemma1)\n",
    "                input_layers.append(input_lemma2)\n",
    "            \n",
    "        \n",
    "        # define input for additional features\n",
    "        # note: dist is restricted to [0 ... 7] so 8 values\n",
    "        if featureset == 'f1':\n",
    "            \"\"\" S.0.POS\n",
    "                B.0.POS\n",
    "                DIST\"\"\"\n",
    "            dim_features = len(vocabs['POS']) * 2 + 8\n",
    "        elif featureset == 'f2':\n",
    "            \"\"\" S.0.POS\n",
    "                S.0.LEMMA   # embeddings\n",
    "                S.0.MORPHO\n",
    "                S.-1.POS\n",
    "                B.0.POS\n",
    "                B.0.LEMMA   # embeddings\n",
    "                B.0.MORPHO\n",
    "                B.-1.POS\n",
    "                B.1.POS\n",
    "                DIST\"\"\"\n",
    "            dim_features = len(vocabs['POS']) * 4  + len(vocabs['MORPHO']) * 2 + 8\n",
    "        else:\n",
    "            \"\"\" S.0.POS\n",
    "                S.0.LEMMA   # embeddings\n",
    "                S.0.MORPHO\n",
    "                S.-1.POS\n",
    "                B.0.POS\n",
    "                B.0.LEMMA   # embeddings\n",
    "                B.0.MORPHO\n",
    "                B.-2.POS\n",
    "                B.-1.POS\n",
    "                B.1.POS\n",
    "                DIST\"\"\"\n",
    "            # same size as 'f2'\n",
    "            dim_features = len(vocabs['POS']) * 5 + len(vocabs['MORPHO']) * 2 + 8\n",
    "        print(\"  expecting {n} features dim\".format(n=dim_features))\n",
    "        \n",
    "        # define input for features\n",
    "        features_input = Input(shape=(dim_features,))\n",
    "        concat_layers.append(features_input)\n",
    "        input_layers.append(features_input)\n",
    "        \n",
    "        # concatenate inputs if there's more than one\n",
    "        if len(concat_layers) > 1:\n",
    "            l = concatenate(concat_layers)\n",
    "        else:\n",
    "            l = features_input\n",
    "            \n",
    "        #if embeddings is not None:\n",
    "            # concatenate features and embeddings\n",
    "            #l = concatenate([embeddings_1, embeddings_2, features_input])\n",
    "        #else:\n",
    "            #l = features_input\n",
    "        \n",
    "        # adding dense layers\n",
    "        \n",
    "        l = Dense(hidden_dim)(l)\n",
    "        l = Activation('relu')(l)\n",
    "        if dropout:\n",
    "            l = Dropout(0.15)(l)\n",
    "        l = Dense(int(hidden_dim / 2))(l)\n",
    "        l = Activation('relu')(l)\n",
    "        if dropout:\n",
    "            l = Dropout(0.15)(l)\n",
    "        \n",
    "        l = Dense(nb_classes)(l)\n",
    "        out = Activation('softmax')(l)\n",
    "        \n",
    "        if len(input_layers) > 1:\n",
    "            net_model = Model(input_layers, out)\n",
    "        else:\n",
    "            net_model = Model(features_input, out)\n",
    "        \n",
    "        # not sure where to compile the model ...\n",
    "        net_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            \n",
    "        if net_model:\n",
    "            self.networks_[network_name] = net_model\n",
    "            # save initial state of this network (this also saves embedding layers, so we remove them)\n",
    "            self.save_network(network_name)\n",
    "            #del self.embeddings_for_words_\n",
    "            #del self.embeddings_for_lemmas_\n",
    "            self.current_network_ = network_name\n",
    "        return net_model\n",
    "\n",
    "    def save_network(self, network_name):\n",
    "        if network_name in self.networks_:\n",
    "            self.networks_[network_name].save(os.path.join(self.path_, network_name + '-net.h5'))\n",
    "        else:\n",
    "            print(\"Net \" + network_name + \" not found\")\n",
    "            \n",
    "    def load_network(self, network_name):\n",
    "        \"\"\"\n",
    "        Loads a keras network model (.h5, architecture and weights) from disk.\n",
    "        \"\"\"\n",
    "        fname = os.path.join(self.path_, network_name + '-net.h5')\n",
    "        print(\"trying to load\", fname)\n",
    "        if os.path.isfile(fname):\n",
    "            try:\n",
    "                self.networks_[network_name] = load_model(fname)\n",
    "                self.current_network_ = network_name\n",
    "                print(\"succesfully loaded network \", network_name, \"from file\", fname)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(\"Could not load keras model because \", str(e))\n",
    "                return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_network(self, network_name):\n",
    "        if network_name in self.networks_:\n",
    "            return self.networks_[network_name]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_current_network(self):\n",
    "        if self.current_network_ in self.networks_:\n",
    "            return self.networks_[self.current_network_]\n",
    "\n",
    "    def remove_network(self, network_name):\n",
    "        \"\"\"\n",
    "        Removes a network model from DependencyClassifier (not from disk !), if it exists it is returned by this method.\n",
    "        \"\"\"\n",
    "        if network_name in self.networks_.keys():\n",
    "            self.current_network_ = None\n",
    "            return self.networks_.pop(model_name, None)    \n",
    "    \n",
    "    def preprocess_embeddings(self, model_name, augment_vocabs=True):\n",
    "        \"\"\"\n",
    "        Preprocess embeddings for training a network for this model.\n",
    "        Notes:\n",
    "        - should be run once BEFORE creating a network, if you want to use pre-trained embeddings to train this network, \n",
    "          if not embeddings will be completely learned during training\n",
    "        - loaded embeddings should be deleted before loading/running a new network to avoid exhausting memory.\n",
    "        - if called again for another model embeddings will be REPLACED\n",
    "        - for already created network embedded remain and will be saved along with the network (no need to call this if\n",
    "          you load a saved network)\n",
    "            - at network creation, once net model is saved to disk with keras model.save(), loaded embeddings are deleted\n",
    "              to free memory\n",
    "            - to manually remove loaded embeddings, delete attributes embeddings_, embeddings_for_words_ and embeddings_for_lemmas_\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            Name of the TAL model.\n",
    "            \n",
    "        Returns: nothing\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        model = self.models_[model_name]\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model['use_forms']\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        \n",
    "        print(\"preprocess_embeddings(model\",model_name,',augment',augment_vocabs,\n",
    "              ',f',featureset,',forms',use_forms,',lang',lang,')')\n",
    "        \n",
    "        \"\"\"model f1_fr ,augment True ,f f1 ,forms False ,lang fr )\"\"\"\n",
    "        \n",
    "        if (self.embeddings_for_words_ is None and use_forms) or (self.embeddings_for_lemmas_ is None and featureset != 'f1'):\n",
    "            print(\"self.embeddings_for_words_ is None\", self.embeddings_for_words_ is None)\n",
    "            print(\"use_forms\", use_forms)\n",
    "            print(\"self.embeddings_for_lemmas_ is None\",self.embeddings_for_lemmas_ is None)\n",
    "            print(\"featureset is not 'f1'\",featureset is not 'f1')\n",
    "            print(\"featureset == 'f1'\", featureset == 'f1')\n",
    "            if self.embeddings_ is None:\n",
    "                print(\"preprocess_embeddings: loading original embeddings ...\")\n",
    "                self.embeddings_, self.apax_ = self.dm_.load_embeddings(lang)\n",
    "        \n",
    "        if self.embeddings_for_words_ is None and use_forms:\n",
    "            faligned = os.path.join(self.path_, 'cache', model_name + '-embeddings-forms-aligned.pkl')\n",
    "            if os.path.isfile(faligned):\n",
    "                self.embeddings_for_words_ = self.dm_.pickle_load(faligned)\n",
    "            if self.embeddings_for_words_ is None:\n",
    "                print(\"preprocess_embeddings: aligning embeddings for forms ...\")\n",
    "                self.embeddings_for_words_, words_not_found, words_matched = self.dm_.align_embeddings(\n",
    "                    vocabs['WORDS'], self.embeddings_, augment_vocab=augment_vocabs, max_vocab_size=self.MAX_VOCAB_SIZE)\n",
    "                self.dm_.safe_pickle_dump(faligned, self.embeddings_for_words_)\n",
    "                    \n",
    "        if self.embeddings_for_lemmas_ is None and featureset is not 'f1':\n",
    "            faligned = os.path.join(self.path_, 'cache', model_name + '-embeddings-lemmas-aligned.pkl')\n",
    "            if os.path.isfile(faligned):\n",
    "                self.embeddings_for_lemmas_ = self.dm_.pickle_load(faligned)\n",
    "            if self.embeddings_for_lemmas_ is None and 'LEMMA' in vocabs.keys():\n",
    "                print(\"preprocess_embeddings: aligning embeddings for lemmas ...\")\n",
    "                self.embeddings_for_lemmas_, words_not_found, words_matched = self.dm_.align_embeddings(\n",
    "                    vocabs['LEMMA'], self.embeddings_, augment_vocab=augment_vocabs, max_vocab_size=self.MAX_VOCAB_SIZE)\n",
    "                self.dm_.safe_pickle_dump(faligned, self.embeddings_for_lemmas_)\n",
    "       \n",
    "        # free some memory - original embeddings should be useless now\n",
    "        if self.embeddings_ is not None:\n",
    "            del self.embeddings_\n",
    "        \n",
    "        return\n",
    "    \n",
    "            \n",
    "    def preprocess_data(self, model_name, X_train, y_train, X_dev, y_dev, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Preprocess data for training a network for this model.\n",
    "        - merges vocabs (WORDS) of X_dev into X_train\n",
    "        - merges vocabs (LEMMA) of X_dev into X_train, if any\n",
    "        - merges vocabs (MORPHO) of X_dev into X_train, if any\n",
    "        - merges vocabs (labels) of Y_dev and Y_test into Y_train\n",
    "        - converts to categorical (one-hot), features of X_* (POS, MORPHO, DIST)\n",
    "        \n",
    "        Note: arrays and vocabs are updated in place, so keep a copy of originals if required.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            TAL model to use for treating data (defines features and vocabs).\n",
    "            \n",
    "        X_train, y_train, X_dev, y_dev, X_test, y_test: arrays\n",
    "            Training, validation and test data to process.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        model = self.models_[model_name]\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model['use_forms']\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        vocabs_dev = model['vocabs_dev']\n",
    "        vocabs_test = model['vocabs_test']\n",
    "        \n",
    "        # Treat vocabularies\n",
    "        # Note: X_test/y_test are used only to manually check relevance of the classifier with \"fake\" arc-eager\n",
    "        # but they are not used to generate test results\n",
    "        \n",
    "        # trim data that was wrongly parsed (non-projective sentences)\n",
    "        print(\"preprocess_data: removing non-projective samples ...\")\n",
    "        print(\"    before \", X_train.shape, y_train.shape)\n",
    "        X_good = X_train[:, -1] == 1\n",
    "        X_train = X_train[X_good]\n",
    "        y_train = y_train[X_good]\n",
    "        print(\"    after \", X_train.shape, y_train.shape)\n",
    "        print(\"    before \", X_dev.shape, y_dev.shape)\n",
    "        X_good = X_dev[:, -1] == 1\n",
    "        X_dev = X_dev[X_good]\n",
    "        y_dev = y_dev[X_good]\n",
    "        print(\"    after \", X_dev.shape, y_dev.shape)\n",
    "        \n",
    "        if use_forms:\n",
    "            print(\"preprocess_data: merging WORDS vocabs from dev into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['WORDS'], vocab2=vocabs_dev['WORDS'], data=X_dev, columns=(0, 1))\n",
    "            # ... vocabs_dev['WORDS'] is now useless\n",
    "            print(\"preprocess_data: aligning WORDS vocabs from test into train ...\")            \n",
    "            # handle test set (align vocabs and set unknown words)\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['WORDS'], vocab2=vocabs_test['WORDS'], data=X_test, columns=(0, 1),\n",
    "                                 test_mode=True) # !!!\n",
    "        \n",
    "        if 'POS' in vocabs:\n",
    "            nb_classes_pos = len(np.unique(vocabs['POS']))\n",
    "        if 'MORPHO' in vocabs:\n",
    "            print(\"preprocess_data: merging and aligning MORPHO vocabs from dev into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['MORPHO'], vocab2=vocabs_dev['MORPHO'], data=X_dev, columns=(4, 8))\n",
    "            # ... vocabs_dev['MORPHO'] is now useless\n",
    "            nb_classes_morpho = len(np.unique(vocabs['MORPHO']))\n",
    "            print(\"preprocess_data: aligning MORPHO vocabs from test into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['MORPHO'], vocab2=vocabs_test['MORPHO'], data=X_test, columns=(4, 8),\n",
    "                                 test_mode=True) # there should be no UNK in this vocab normally !\n",
    "                                                \n",
    "        if 'LEMMA' in vocabs:\n",
    "            print(\"preprocess_data: merging and aligning LEMMA vocabs from dev into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['LEMMA'], vocab2=vocabs_dev['LEMMA'], data=X_dev, columns=(3, 7))\n",
    "            # ... vocabs_dev['LEMMA'] is now useless            \n",
    "            print(\"preprocess_data: aligning LEMMA vocabs from test into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['LEMMA'], vocab2=vocabs_test['LEMMA'], data=X_test, columns=(3, 7),\n",
    "                                 test_mode=True) # !!!\n",
    "\n",
    "        print(\"preprocess_data: merging and aligning LABEL vocabs from dev into train ...\")\n",
    "        self.dm_.merge_vocabs(vocab1=vocabs['LABELS'], vocab2=vocabs_dev['LABELS'], data=y_dev, columns=(0,))\n",
    "        print(\"preprocess_data: merging and aligning LABEL vocabs from test into train ...\")\n",
    "        self.dm_.merge_vocabs(vocab1=vocabs['LABELS'], vocab2=vocabs_test['LABELS'], data=y_test, columns=(0,),\n",
    "                             test_mode=True) # there should be no UNK in this vocab !\n",
    "        nb_classes = len(vocabs['LABELS'])\n",
    "            \n",
    "        # convert some features to one-hot encoding\n",
    "        \n",
    "        nb_classes_dist = 8\n",
    "        \n",
    "        \n",
    "        print(\"preprocess_data: converting {f} features to one-hot encoding...\".format(f=featureset))\n",
    "        \n",
    "        if featureset == 'f1':\n",
    "            \"\"\"\n",
    "            2 S.0.POS\n",
    "            3 B.0.POS\n",
    "            4 DIST\"\"\"\n",
    "            cats_pos1 = to_categorical(X_train[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(X_train[:, 3], num_classes=nb_classes_pos)\n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_train[:, 4]), num_classes=nb_classes_dist)\n",
    "            if use_forms:\n",
    "                X_train = np.column_stack((X_train[:, 0], X_train[:, 1], cats_pos1, cats_pos2, cats_dist))\n",
    "            else:\n",
    "                X_train = np.column_stack((cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "            cats_pos1 = to_categorical(X_dev[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(X_dev[:, 3], num_classes=nb_classes_pos)\n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_dev[:, 4]), num_classes=nb_classes_dist)\n",
    "            if use_forms:\n",
    "                X_dev = np.column_stack((X_dev[:, 0], X_dev[:, 1], cats_pos1, cats_pos2, cats_dist))\n",
    "            else:\n",
    "                X_dev = np.column_stack((cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "            cats_pos1 = to_categorical(X_test[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(X_test[:, 3], num_classes=nb_classes_pos)\n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_test[:, 4]), num_classes=nb_classes_dist)\n",
    "            if use_forms:\n",
    "                X_test = np.column_stack((X_test[:, 0], X_test[:, 1], cats_pos1, cats_pos2, cats_dist))\n",
    "            else:\n",
    "                X_test = np.column_stack((cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "        elif featureset == 'f2': # 'f2' and 'f3' featuresets have same structure\n",
    "            \"\"\" \n",
    "            2 S.0.POS\n",
    "            3 S.0.LEMMA\n",
    "            4 S.0.MORPHO\n",
    "            5 S.-1.POS (f2, or S.1.POS for f3)\n",
    "            6 B.0.POS\n",
    "            7 B.0.LEMMA\n",
    "            8 B.0.MORPHO\n",
    "            9 B.-1.POS\n",
    "            10 B.1.POS\n",
    "            11 DIST\"\"\"                \n",
    "            cats_pos1    = to_categorical(X_train[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_train[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_train[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_train[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_train[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_train[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_train[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_train[:, 11]), num_classes=nb_classes_dist)\n",
    "            X_train = np.column_stack((X_train[:, 3], X_train[:, 7], cats_pos1, \n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                                cat_trains_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "            \n",
    "            cats_pos1    = to_categorical(X_dev[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_dev[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_dev[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_dev[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_dev[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_dev[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_dev[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_dev[:, 11]), num_classes=nb_classes_dist)\n",
    "            X_dev = np.column_stack((X_dev[:, 3], X_dev[:, 7], cats_pos1,\n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               cat_trains_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "            \n",
    "\n",
    "            cats_pos1    = to_categorical(X_test[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_test[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_test[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_test[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_test[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_test[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_test[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_test[:, 11]), num_classes=nb_classes_dist)\n",
    "            X_test = np.column_stack((X_test[:, 3], X_test[:, 7], cats_pos1, \n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               cats_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "            \n",
    "        elif featureset == 'f3':\n",
    "            \"\"\" \n",
    "            2 S.0.POS\n",
    "            3 S.0.LEMMA\n",
    "            4 S.0.MORPHO\n",
    "            5 S.-1.POS    (S.1.POS does not exist)\n",
    "            6 B.0.POS\n",
    "            7 B.0.LEMMA\n",
    "            8 B.0.MORPHO\n",
    "            9 B.-2.POS    (added for f3)\n",
    "            10 B.-1.POS\n",
    "            11 B.1.POS\n",
    "            12 DIST\"\"\"                \n",
    "            cats_pos1    = to_categorical(X_train[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_train[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_train[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_train[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_train[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_train[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_train[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_pos6    = to_categorical(X_train[:, 11], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_train[:, 12]), num_classes=nb_classes_dist)\n",
    "            X_train = np.column_stack((X_train[:, 3], X_train[:, 7], cats_pos1, cats_morpho1, \n",
    "                                       cats_pos2, cats_pos3, cat_trains_morpho2, cats_pos4, \n",
    "                                       cats_pos5, cats_pos6, cats_dist))\n",
    "            \n",
    "            cats_pos1    = to_categorical(X_dev[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_dev[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_dev[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_dev[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_dev[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_dev[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_dev[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_pos6    = to_categorical(X_dev[:, 11], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_dev[:, 12]), num_classes=nb_classes_dist)\n",
    "            X_dev = np.column_stack((X_dev[:, 3], X_dev[:, 7], cats_pos1, \n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                                cat_trains_morpho2, cats_pos4, cats_pos5, cats_pos6, cats_dist))\n",
    "            \n",
    "\n",
    "            cats_pos1    = to_categorical(X_test[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_test[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_test[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_test[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_test[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_test[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_test[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_pos6    = to_categorical(X_test[:, 11], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_test[:, 12]), num_classes=nb_classes_dist)\n",
    "            X_test = np.column_stack((X_test[:, 3], X_test[:, 7], cats_pos1, \n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               cats_morpho2, cats_pos4, cats_pos5, cats_pos6, cats_dist))            \n",
    "                        \n",
    "        print(\"preprocess_data: converting LABELs to one-hot encoding ...\")\n",
    "        y_train = to_categorical(y_train, num_classes=nb_classes)\n",
    "        y_dev = to_categorical(y_dev, num_classes=nb_classes)\n",
    "        y_test = to_categorical(y_test, num_classes=nb_classes)\n",
    "                \n",
    "        return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "    \n",
    "\n",
    "    def process_test_data(self, X_test):\n",
    "        \"\"\"\n",
    "        Process one sample data in order to fit it to the keras model.\n",
    "        Format should be a vector with same list of items (POS, MORPHO, LEMMA, optionnally FORM ...), but with their\n",
    "        VALUES (and not their index in a vocab).\n",
    "        If there are additional columns in input they will not be used (nor returned).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        X_test: array/list\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        transformed X_test: array/list that can be passed to predict\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        w1[FORM , POS , GOV , LABEL]\n",
    "        w2[FORM , POS , GOV , LABEL]\n",
    "        \n",
    "        f2\n",
    "        FORM , POS , GOV , LABEL, LEMMA , MORPHO\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"process_data({data})\".format(data=X_test))\n",
    "        \n",
    "        model = self.get_current_model()\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model['use_forms']\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        vocabs_dev = model['vocabs_dev']\n",
    "        vocabs_test = model['vocabs_test']\n",
    "        \n",
    "        nb_classes = len(vocabs['LABELS'])\n",
    "        nb_classes_pos = len(vocabs['POS'])\n",
    "        if 'MORPHO' in vocabs.keys():\n",
    "            nb_classes_morpho = len(vocabs['MORPHO'])\n",
    "        nb_classes_dist = 8\n",
    "        \n",
    "        unknown_idx = -1\n",
    "        unknown_lemma_idx = -1\n",
    "        unknown_morpho_idx = -1\n",
    "        if self.UNKNOWN_WORD in vocabs['WORDS']:\n",
    "            unknown_idx = vocabs['WORDS'].index(self.UNKNOWN_WORD)\n",
    "        if 'LEMMA' in vocabs.keys():\n",
    "            if self.UNKNOWN_WORD in vocabs['LEMMA']:\n",
    "                unknown_lemma_idx = vocabs['LEMMA'].index(self.UNKNOWN_WORLD)\n",
    "        if 'MORPHO' in vocabs.keys():\n",
    "            if self.UNKNOWN_WORD in vocabs['MORPHO']:\n",
    "                unknown_morpho_idx = vocabs['MORPHO'].index(self.UNKNOWN_WORLD)\n",
    "        \n",
    "        # align to vocabs indices (the ones known by the keras network)\n",
    "        if featureset == 'f1':\n",
    "            \"\"\"\n",
    "            2 S.0.POS\n",
    "            3 B.0.POS\n",
    "            4 DIST\"\"\"\n",
    "            cats_pos1 = to_categorical(vocabs['POS'].index(X_test[2]), num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(vocabs['POS'].index(X_test[3]), num_classes=nb_classes_pos)           \n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_test[4]), num_classes=nb_classes_dist)\n",
    "            \n",
    "            if use_forms:\n",
    "                if X_test[0] in vocabs['WORDS']:\n",
    "                    w1_idx = vocabs['WORDS'].index(X_test[0])\n",
    "                else:\n",
    "                    w1_idx = unknown_idx\n",
    "                if X_test[1] in vocabs['WORDS']:\n",
    "                    w2_idx = vocabs['WORDS'].index(X_test[1])\n",
    "                else:\n",
    "                    w2_idx = unknown_idx\n",
    "                    \n",
    "                X_test = np.asarray((w1_idx, w2_idx, cats_pos1, cats_pos2, cats_dist))\n",
    "            else:\n",
    "                X_test = np.asarray((cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "        elif featureset == 'f2':\n",
    "            \"\"\" \n",
    "            2 S.0.POS\n",
    "            3 S.0.LEMMA\n",
    "            4 S.0.MORPHO\n",
    "            5 S.-1.POS (f2, or S.1.POS for f3)\n",
    "            6 B.0.POS\n",
    "            7 B.0.LEMMA\n",
    "            8 B.0.MORPHO\n",
    "            9 B.-1.POS\n",
    "            10 B.1.POS\n",
    "            11 DIST\"\"\"        \n",
    "            cats_pos1    = to_categorical(vocabs['POS'].index(X_test[2]), num_classes=nb_classes_pos)\n",
    "            if X_test[4] in vocabs['MORPHO']:\n",
    "                m_idx = vocabs['MORPHO'].index(X_test[4])\n",
    "            else:\n",
    "                m_idx = unknown_morpho_idx\n",
    "            cats_morpho1 = to_categorical(m_idx, num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(vocabs['POS'].index(X_test[5]), num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(vocabs['POS'].index(X_test[6]), num_classes=nb_classes_pos)\n",
    "            if X_test[8] in vocabs['MORPHO']:\n",
    "                m_idx = vocabs['MORPHO'].index(X_test[8])\n",
    "            else:\n",
    "                m_idx = unknown_morpho_idx            \n",
    "            cats_morpho2 = to_categorical(m_idx, num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(vocabs['POS'].index(X_test[9]), num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(vocabs['POS'].index(X_test[10]), num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_train[11]), num_classes=nb_classes_dist)\n",
    "            if X_test[3] in vocabs['LEMMA']:\n",
    "                l1_idx = vocabs['LEMMA'].index(X_test[3])\n",
    "            else:\n",
    "                l1_idx = unknown_lemma_idx\n",
    "            if X_test[7] in vocabs['LEMMA']:\n",
    "                l2_idx = vocabs['LEMMA'].index(X_test[7])\n",
    "            else:\n",
    "                l2_idx = unknown_lemma_idx\n",
    "            # lemmas (embeddings) are set at beginning of inputs\n",
    "            X_train = np.asarray((l1_idx, l2_idx, cats_pos1, cats_morpho1, cats_pos2, cats_pos3,\n",
    "                                       cat_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "            \n",
    "        elif featureset == 'f3':\n",
    "            \"\"\" \n",
    "            2 S.0.POS\n",
    "            3 S.0.LEMMA\n",
    "            4 S.0.MORPHO\n",
    "            5 S.-1.POS    (S.1.POS does not exist)\n",
    "            6 B.0.POS\n",
    "            7 B.0.LEMMA\n",
    "            8 B.0.MORPHO\n",
    "            9 B.-2.POS    (added for f3)\n",
    "            10 B.-1.POS\n",
    "            11 B.1.POS\n",
    "            12 DIST\"\"\"  \n",
    "            cats_pos1    = to_categorical(vocabs['POS'].index(X_test[2]), num_classes=nb_classes_pos)\n",
    "            if X_test[4] in vocabs['MORPHO']:\n",
    "                m_idx = vocabs['MORPHO'].index(X_test[4])\n",
    "            else:\n",
    "                m_idx = unknown_morpho_idx\n",
    "            cats_morpho1 = to_categorical(m_idx, num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(vocabs['POS'].index(X_test[5]), num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(vocabs['POS'].index(X_test[6]), num_classes=nb_classes_pos)\n",
    "            if X_test[8] in vocabs['MORPHO']:\n",
    "                m_idx = vocabs['MORPHO'].index(X_test[8])\n",
    "            else:\n",
    "                m_idx = unknown_morpho_idx            \n",
    "            cats_morpho2 = to_categorical(m_idx, num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(vocabs['POS'].index(X_test[9]), num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(vocabs['POS'].index(X_test[10]), num_classes=nb_classes_pos)\n",
    "            cats_pos6    = to_categorical(vocabs['POS'].index(X_test[11]), num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_train[12]), num_classes=nb_classes_dist)\n",
    "            if X_test[3] in vocabs['LEMMA']:\n",
    "                l1_idx = vocabs['LEMMA'].index(X_test[3])\n",
    "            else:\n",
    "                l1_idx = unknown_lemma_idx\n",
    "            if X_test[7] in vocabs['LEMMA']:\n",
    "                l2_idx = vocabs['LEMMA'].index(X_test[7])\n",
    "            else:\n",
    "                l2_idx = unknown_lemma_idx\n",
    "            # lemmas (embeddings) are set at beginning of inputs\n",
    "            X_test = np.asarray((l1_idx, l2_idx, cats_pos1, cats_morpho1, cats_pos2, cats_pos3,\n",
    "                                       cat_morpho2, cats_pos4, cats_pos5, cats_pos6, cats_dist))            \n",
    "            \n",
    "                \n",
    "        return X_test\n",
    "    \n",
    "    def get_label(self, y_pred):\n",
    "        y_pred_idx = np.argmax(y_pred)\n",
    "        return self.get_current_model()['vocabs']['LABELS'][y_pred_idx]\n",
    "    \n",
    "    def print_data(self, model_name, X_, Y, idx, vocab_type):\n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        \n",
    "        model = self.models_[model_name]\n",
    "        featureset = model['featureset']\n",
    "        lang = model['lang']\n",
    "        vocabs = model[vocab_type]\n",
    "        \n",
    "        X = X_[idx]\n",
    "        \n",
    "        print(\"feat\", featureset)\n",
    "            \n",
    "        if featureset == 'f1':\n",
    "            print(vocabs['WORDS'][X[0]], vocabs['WORDS'][X[1]], \n",
    "                  vocabs['POS'][X[2]], vocabs['POS'][X[3]], X[4], '==>', vocabs['LABELS'][Y[idx]])\n",
    "\n",
    "        elif featureset == 'f2':\n",
    "            print(vocabs['WORDS'][X[0]], vocabs['WORDS'][X[1]], \n",
    "                  vocabs['POS'][X[2]], vocabs['LEMMA'][X[3]], vocabs['MORPHO'][X[4]],\n",
    "                  vocabs['POS'][X[5]], vocabs['POS'][X[6]] ,vocabs['LEMMA'][X[7]], vocabs['MORPHO'][X[8]],\n",
    "                  vocabs['POS'][X[9]], vocabs['POS'][X[10]],  X[11], '==>', vocabs['LABELS'][Y[idx]])\n",
    "        elif featureset == 'f3':\n",
    "            print(vocabs['WORDS'][X[0]], vocabs['WORDS'][X[1]], \n",
    "                  vocabs['POS'][X[2]], vocabs['LEMMA'][X[3]], vocabs['MORPHO'][X[4]],\n",
    "                  vocabs['POS'][X[5]], vocabs['POS'][X[6]] ,vocabs['LEMMA'][X[7]], vocabs['MORPHO'][X[8]],\n",
    "                  vocabs['POS'][X[9]], vocabs['POS'][X[10]], vocabs['POS'][X[11]], X[11], '==>', vocabs['LABELS'][Y[idx]])\n",
    "\n",
    "            \n",
    "    def print_status(self):\n",
    "        print()\n",
    "        print(\"=== DependencyClassifier = max_vocab_size {max} unknown form {unk} ===\".format(max=self.MAX_VOCAB_SIZE,\n",
    "                                                                                             unk=self.UNKNOWN_WORD))\n",
    "        for model in self.models_:\n",
    "            print(\"   {name} -- {mod}\".format(name=model, mod=[\"{k}:{v},\".format(k=k,v=v) for k,v in self.models_[model].items()]))\n",
    "        for net in self.networks_:\n",
    "            print(net)\n",
    "            \n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (Arc_eager.py, line 467)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\jerem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2963\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-4-857fb7ce8914>\"\u001b[0m, line \u001b[0;32m3\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    import generate_data as aegen\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\IAAA\\tal-github\\src\\generate_data.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from Arc_eager import *\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\IAAA\\tal-github\\src\\Arc_eager.py\"\u001b[1;36m, line \u001b[1;32m467\u001b[0m\n\u001b[1;33m    X_test_i = oracle.process_test_data(x_test)\u001b[0m\n\u001b[1;37m                                               ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method Beeper.post_execute of <__main__.Beeper object at 0x000002620E495588>> (for post_execute):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ab19bcbf9860>\u001b[0m in \u001b[0;36mpost_execute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpost_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_time\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_time\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInvisibleAudio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautoplay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import importlib \n",
    "import time\n",
    "import generate_data as aegen\n",
    "\n",
    "importlib.reload(aegen)\n",
    "\n",
    "def STUB(class_mgr):\n",
    "    \"\"\"\n",
    "    Arc eager on keras model, using vocab_test (aligned to keras network embeddings), writing conllu file\n",
    "    as fname.conllu.\n",
    "    \"\"\"\n",
    "    keras_model = class_mgr.get_current_network()\n",
    "    \n",
    "    model = class_mgr.get_current_model()\n",
    "    featureset = model['featureset']\n",
    "    lang = model['lang']\n",
    "    \n",
    "    print(\"ArcEager: generate test results for {lang} / {featureset}\".format(lang=lang, featureset=featureset))\n",
    "    \n",
    "    if featureset == 'f1':\n",
    "        if model['use_forms']:\n",
    "            #X_test = [w1, w2, s0pos, b0pos, dist]\n",
    "            pass\n",
    "        else:\n",
    "            #X_test = [s0pos, b0pos, dist]\n",
    "            pass\n",
    "    elif featureset == 'f2':\n",
    "        # ...\n",
    "        pass\n",
    "    else:\n",
    "        # ...\n",
    "        pass\n",
    "    \n",
    "    # align to neural network input formats\n",
    "    #X_test = class_mgr.process_test_data(X_test)\n",
    "    #y_pred = keras_model.predict(X_test)\n",
    "    #y = keras_model.get_label(y_pred) # converts from one-hot to label string \"RIGHT_det\", using vocab\n",
    "    \n",
    "    # ...\n",
    "        \n",
    "    \n",
    "\n",
    "def main(epochs=10, max_vocab_size=100000, unknown_word='<UNK>'):\n",
    "\n",
    "    dm = DataManager('../')\n",
    "    # define a manager with hard limit of 100000 for vocabularies lengths\n",
    "    dep_classifier = DependencyClassifier(path='../', max_vocab_size=100000, unknown_word='<UNK>')\n",
    "    \n",
    "    test_info = {'fr': os.path.join(\"..\", \"UD_French-GSD\", \"fr_gsd-ud-test.conllu\"),       \n",
    "                 'nl' : os.path.join(\"..\", \"UD_Dutch-LassySmall\", \"nl_lassysmall-ud-test.conllu\") ,              \n",
    "                 'en' : os.path.join(\"..\", \"UD_English-LinES\", \"en_lines-ud-test.conllu\") ,                              \n",
    "                 'ja' : os.path.join(\"..\", \"UD_Japanese-GSD\", \"ja_gsd-ud-test.conllu\")\n",
    "                }\n",
    "\n",
    "    for lang in ['fr', 'nl', 'en', 'ja']:\n",
    "    \n",
    "        for featureset in ['f1', 'f1-forms', 'f2', 'f3']:\n",
    "            \n",
    "            test_results_fname = \"{featureset}_{lang}_results.conllu\"\n",
    "    \n",
    "            if os.path.isfile(test_results_fname):\n",
    "                print(\"{f} already exists, skipping this task\".format(f=test_results_fname))\n",
    "                break\n",
    "    \n",
    "            # === CREATE NEURAL NETWORK ===\n",
    "            print()\n",
    "            print(\"========= {lang} === {featureset} ==========\".format(lang=lang, featureset=featureset))\n",
    "            print()\n",
    "            print(\"===== CREATING NEURAL NETWORK =====\")\n",
    "            print()\n",
    "            t = time.time()\n",
    "            print(\"= Loading data...\")\n",
    "            feat = featureset\n",
    "            if featureset == 'f1-forms':\n",
    "                feat = 'f1'\n",
    "            X_train, y_train, vocabs_train = dm.load_data('train', lang, feat)\n",
    "            X_dev, y_dev, vocabs_dev = dm.load_data('dev', lang, feat)\n",
    "            X_test, y_test, vocabs_test = dm.load_data('test', lang, feat)\n",
    "\n",
    "            print(\"  ... loaded data in \", time.time() - t)\n",
    "\n",
    "            nb_classes = len(vocabs_train['LABELS'])\n",
    "\n",
    "            model_name = \"{featureset}_{lang}\".format(featureset=featureset, lang=lang)\n",
    "            \n",
    "            if featureset is 'f1-forms':\n",
    "                model_name = model_name + '-forms'\n",
    "\n",
    "            # create a TAL model for this combination\n",
    "            exists = dep_classifier.load_model(model_name)\n",
    "            if exists and dep_classifier.get_model_status(model_name):\n",
    "                print(\"==> Neural network for this model already prepared\")\n",
    "            else:\n",
    "                dep_classifier.create_model(model_name, lang, feat, featureset is 'f1-forms', \n",
    "                                            vocabs_train, vocabs_dev, vocabs_test)\n",
    "                dep_classifier.save_model(model_name)\n",
    "\n",
    "            if dep_classifier.get_model_test_status(model_name):\n",
    "                print(\"==> Test conllu file already generated, skipping...\")\n",
    "                break\n",
    "                \n",
    "            t = time.time()\n",
    "\n",
    "            print(\"= Pre-processing data ...\")\n",
    "            # preprocess X/Y data\n",
    "            mod = dep_classifier.get_current_model()\n",
    "            idx_test = 40\n",
    "            w = mod['vocabs_test']['WORDS'][X_test[idx_test][0]]\n",
    "            print(\"test sample \", idx_test, 'id ', X_test[idx_test][0], '=', w)\n",
    "            idx = mod['vocabs']['WORDS'].index(w)\n",
    "            for x in range(len(X_train)):\n",
    "                if X_train[x][0] == idx:\n",
    "                    idx_train = x\n",
    "                    break\n",
    "            idx = mod['vocabs_dev']['WORDS'].index(w)\n",
    "            for x in range(len(X_dev)):\n",
    "                if X_dev[x][0] == idx:\n",
    "                    idx_dev = x\n",
    "                    break\n",
    "            print(\"x_train\", X_train[idx_train])\n",
    "            print(\"y_train\", y_train[idx_train])\n",
    "            print(\"x_dev\", X_dev[idx_dev])\n",
    "            print(\"y_dev\", y_dev[idx_dev])\n",
    "            print(\"x_test\", X_test[idx_test])\n",
    "            print(\"y_test\", y_test[idx_test])\n",
    "            dep_classifier.print_data(model_name, X_train, y_train, idx_train, 'vocabs')\n",
    "            dep_classifier.print_data(model_name, X_dev, y_dev, idx_dev, 'vocabs_dev')\n",
    "            dep_classifier.print_data(model_name, X_test, y_test, idx_test, 'vocabs_test')\n",
    "            X_train, y_train, X_dev, y_dev, X_test, y_test = dep_classifier.preprocess_data(model_name, \n",
    "                                                                                X_train, y_train, \n",
    "                                                                                X_dev, y_dev, \n",
    "                                                                                X_test, y_test)\n",
    "\n",
    "            print(\"  ... preprocessed data in \", time.time() - t)\n",
    "            print(\"x_train\", X_train[idx_train])\n",
    "            print(\"y_train\", y_train[idx_train])\n",
    "            print(\"x_dev\", X_dev[idx_dev])\n",
    "            print(\"y_dev\", y_dev[idx_dev])\n",
    "            print(\"x_test\", X_test[idx_test])\n",
    "            print(\"y_test\", y_test[idx_test])        \n",
    "            t = time.time()\n",
    "            print(\"= Preprocessing embeddings ...\")\n",
    "            # load pretrained embeddings\n",
    "            # embeddings have already been processed\n",
    "            dep_classifier.preprocess_embeddings(model_name, augment_vocabs=True)\n",
    "\n",
    "            print(\"  ... preprocessed embeddings in \", time.time() - t)\n",
    "\n",
    "            t = time.time()\n",
    "            print(\"= Creating neural network architecture ...\")\n",
    "            # create a classifier for this TAL model\n",
    "            network_name = \"{model}\".format(model=model_name)\n",
    "            existsnet = dep_classifier.load_network(network_name)\n",
    "            net = None\n",
    "            if not existsnet:\n",
    "                dep_classifier.create_network(network_name, model_name, nb_classes=nb_classes, dropout=True)\n",
    "                if net is None:\n",
    "                    print(\"  ==> could not create network architecture for this model !!!\")\n",
    "            else:\n",
    "                print(\"network architecture already exists, loaded from disk\")\n",
    "            net = dep_classifier.get_network(network_name)\n",
    "            \n",
    "            print()\n",
    "            print(net.summary())\n",
    "            print()\n",
    "\n",
    "            print(\"  ... created net in \", time.time() - t)\n",
    "\n",
    "            #dep_classifier.print_status()\n",
    "            \n",
    "            print()\n",
    "            print(\"===== TRAINING NEURAL NETWORK =====\")\n",
    "            print()           \n",
    "        \n",
    "        \n",
    "            #@TODO currently if recalled network will be trained again for epochs if it already exists\n",
    "            print(\"= Training for {epochs} epochs ...\".format(epochs=epochs))\n",
    "            inputs = []\n",
    "            inputs_dev = []\n",
    "            inputs_test = []\n",
    "            if featureset == 'f1':\n",
    "                inputs = [X_train]\n",
    "                inputs_dev = [X_dev]\n",
    "                inputs_test = [X_test]\n",
    "            else:\n",
    "                inputs = [X_train[:, 0], X_train[:, 1], X_train[:, 2:X_train.shape[1]]]\n",
    "                inputs_dev = [X_dev[:, 0], X_dev[:, 1], X_dev[:, 2:X_dev.shape[1]]]\n",
    "                inputs_test = [X_test[:, 0], X_test[:, 1], X_test[:, 2:X_test.shape[1]]]\n",
    "                \n",
    "            if net is not None:\n",
    "                history = net.fit(inputs, y_train, validation_data=(inputs_dev, y_dev), batch_size=128, epochs=epochs)\n",
    "            else:\n",
    "                print(\"problem : net is none\")\n",
    "            \n",
    "            # save the network\n",
    "            print(\"= Saving network architecture ...\")\n",
    "            dep_classifier.save_network(network_name)\n",
    "            \n",
    "            dep_classifier.set_model_done(model_name)\n",
    "            dep_classifier.save_model(model_name)            \n",
    "            \n",
    "        \n",
    "            #@TODO plot history ?\n",
    "            print(\"= Evaluating ...\")\n",
    "            results = net.evaluate(inputs_test, y_test, batch_size=128)\n",
    "            print(\"  ==> (temporary) results for {lang} / {featureset}: \", results)\n",
    "            \n",
    "            print()\n",
    "            print(\"===== GENERATING TEST RESULTS =====\")\n",
    "            #STUB(dep_classifier)\n",
    "            \n",
    "            print(\"= Calling Arc Eager with new oracle ...\")\n",
    "            if not dep_classifier.get_model_test_status(model_name):\n",
    "                result_path = os.path.dirname(os.path.abspath(test_info[lang]))\n",
    "                result_name = os.path.join(result_path, test_results_fname)\n",
    "                aegen.create_conllu(test_info[lang], feat, result_name, oracle_=dep_classifier)\n",
    "            \n",
    "                print(\"= Setting model as fully done ...\")\n",
    "                # tag this combination as done and save it\n",
    "                dep_classifier.set_model_test_done(model_name)\n",
    "                dep_classifier.save_model(model_name)\n",
    "                \n",
    "            #dep_classifier.remove_network(network_name)\n",
    "            #dep_classifier.remove_model(model_name)\n",
    "\n",
    "main(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= Loading data...\n",
      "pickle.load ../data\\f1_fr-train\n",
      "load_data: loaded X =  (742384, 6) , Y =  (742384,) , vocabs =  WORDS (42280), POS (20), LABELS (91), \n",
      "pickle.load ../data\\f1_fr-dev\n",
      "load_data: loaded X =  (74492, 6) , Y =  (74492,) , vocabs =  WORDS (9277), POS (20), LABELS (75), \n",
      "pickle.load ../data\\f1_fr-test\n",
      "load_data: loaded X =  (20872, 6) , Y =  (20872,) , vocabs =  WORDS (3287), POS (20), LABELS (69), \n",
      "pickle.load ../f1-forms_fr-model.pkl\n",
      "[Errno 2] No such file or directory: '../f1-forms_fr-model.pkl'\n",
      "feat create  f1\n",
      "featureset  f1\n",
      "= Pre-processing data ...\n",
      "test sample  40 id  1 = sens\n",
      "166\n",
      "x_train [1456   25    1    6    1    1]\n",
      "y_train 89\n",
      "x_dev [167 169   0   5   3   1]\n",
      "y_dev 38\n",
      "x_test [ 1 18  1  1  7  1]\n",
      "y_test 43\n",
      "feat f1\n",
      "sens de NOUN ADP 1 ==> SHIFT\n",
      "feat f1\n",
      "Louis né PROPN VERB 3 ==> RIGHT_acl\n",
      "feat f1\n",
      "sens pourrions VERB VERB 7 ==> RIGHT_ccomp\n",
      "preprocess_data: removing non-projective samples ...\n",
      "    before  (742384, 6) (742384,)\n",
      "    after  (661874, 6) (661874,)\n",
      "    before  (74492, 6) (74492,)\n",
      "    after  (66716, 6) (66716,)\n",
      "preprocess_data: merging WORDS vocabs from dev into train ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-ac25c9b84a1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                                     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                                                                     \u001b[0mX_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                                                                     X_test, y_test)\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x_train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-75ad4d31e60b>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[1;34m(self, model_name, X_train, y_train, X_dev, y_dev, X_test, y_test)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_forms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"preprocess_data: merging WORDS vocabs from dev into train ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdm_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_vocabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'WORDS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocabs_dev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'WORDS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;31m# ... vocabs_dev['WORDS'] is now useless\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"preprocess_data: aligning WORDS vocabs from test into train ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-0f967fd51565>\u001b[0m in \u001b[0;36mmerge_vocabs\u001b[1;34m(self, vocab1, vocab2, data, columns, test_mode, verbose)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                         \u001b[1;32mif\u001b[0m \u001b[0mdata_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0midx_vocab2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                                 print(\"Replacing word [{i},{j}]= {idx}, {w} index {fro} to {to}\"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if True:\n",
    "    sys.exit(0)\n",
    "lang='fr'\n",
    "featureset='f1-forms'\n",
    "\n",
    "dm = DataManager('../')\n",
    "# define a manager with hard limit of 100000 for vocabularies lengths\n",
    "dep_classifier = DependencyClassifier(path='../', max_vocab_size=100000, unknown_word='<UNK>')\n",
    "\n",
    "print(\"= Loading data...\")\n",
    "feat = featureset\n",
    "if featureset == 'f1-forms':\n",
    "    feat = 'f1'\n",
    "X_train, y_train, vocabs_train = dm.load_data('train', lang, feat)\n",
    "X_dev, y_dev, vocabs_dev = dm.load_data('dev', lang, feat)\n",
    "X_test, y_test, vocabs_test = dm.load_data('test', lang, feat)\n",
    "\n",
    "nb_classes = len(vocabs_train['LABELS'])\n",
    "\n",
    "model_name = \"{featureset}_{lang}\".format(featureset=featureset, lang=lang)\n",
    "\n",
    "if featureset is 'f1-forms':\n",
    "    model_name = model_name + '-forms'\n",
    "\n",
    "# create a TAL model for this combination\n",
    "exists = dep_classifier.load_model(model_name)\n",
    "if exists and dep_classifier.get_model_status(model_name):\n",
    "    print(\"==> Neural network for this model already prepared\")\n",
    "else:\n",
    "    print(\"feat create \", feat)\n",
    "    dep_classifier.create_model(model_name, lang, feat, featureset == 'f1-forms', \n",
    "                                vocabs_train, vocabs_dev, vocabs_test)\n",
    "\n",
    "    \n",
    "print(\"= Pre-processing data ...\")\n",
    "# preprocess X/Y data\n",
    "mod = dep_classifier.get_current_model()\n",
    "idx_test = 40\n",
    "w = mod['vocabs_test']['WORDS'][X_test[idx_test][0]]\n",
    "print(\"test sample \", idx_test, 'id ', X_test[idx_test][0], '=', w)\n",
    "idx = mod['vocabs']['WORDS'].index(w)\n",
    "for x in range(len(X_train)):\n",
    "    if X_train[x][0] == idx:\n",
    "        idx_train = x\n",
    "        break\n",
    "idx = mod['vocabs_dev']['WORDS'].index(w)\n",
    "print (idx)\n",
    "for x in range(len(X_dev)):\n",
    "    if X_train[x][0] == idx:\n",
    "        idx_dev = x\n",
    "        break\n",
    "print(\"x_train\", X_train[idx_train])\n",
    "print(\"y_train\", y_train[idx_train])\n",
    "print(\"x_dev\", X_dev[idx_dev])\n",
    "print(\"y_dev\", y_dev[idx_dev])\n",
    "print(\"x_test\", X_test[idx_test])\n",
    "print(\"y_test\", y_test[idx_test])\n",
    "dep_classifier.print_data(model_name, X_train, y_train, idx_train, 'vocabs')\n",
    "dep_classifier.print_data(model_name, X_dev, y_dev, idx_dev, 'vocabs_dev')\n",
    "dep_classifier.print_data(model_name, X_test, y_test, idx_test, 'vocabs_test')\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = dep_classifier.preprocess_data(model_name, \n",
    "                                                                    X_train, y_train, \n",
    "                                                                    X_dev, y_dev, \n",
    "                                                                    X_test, y_test)\n",
    "\n",
    "print(\"x_train\", X_train[idx_train])\n",
    "print(\"y_train\", y_train[idx_train])\n",
    "print(\"x_dev\", X_dev[idx_dev])\n",
    "print(\"y_dev\", y_dev[idx_dev])\n",
    "print(\"x_test\", X_test[idx_test])\n",
    "print(\"y_test\", y_test[idx_test])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "toto = 'f1'\n",
    "print(toto == 'f1')\n",
    "print (toto is 'f1')\n",
    "print(toto is not 'f1')\n",
    "print (toto != 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- il n'y a pas les mots dans les features (seulement les lemmes)\n",
    "\n",
    "- ajouter les mots uniquement pour l'expérience en plus\n",
    "\n",
    "- embeddings pour les lemmes: il faudrait ajouter tous les mots des embeddings ! (et pas les enlever lors de l'alignement)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
