{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Licenses\n",
    "GloVe\n",
    "Public Domain Dedication and License v1.0 whose full text can be found at: http://www.opendatacommons.org/licenses/pddl/1.0/\n",
    "\n",
    "Facebookresearch / FastText words embeddings\n",
    "https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "@article{bojanowski2016enriching,\n",
    "  title={Enriching Word Vectors with Subword Information},\n",
    "  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n",
    "  journal={arXiv preprint arXiv:1607.04606},\n",
    "  year={2016}\n",
    "}\n",
    "\n",
    "License Creative Commons Attribution-Share-Alike License 3.0 (https://creativecommons.org/licenses/by-sa/3.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing to unregister  name 'beeper' is not defined\n"
     ]
    }
   ],
   "source": [
    "# optional - plays a sound when a cell completed\n",
    "# note: for any reason this should be executed after keras imports\n",
    "\n",
    "from time import time\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "class InvisibleAudio(Audio):\n",
    "    def _repr_html_(self):\n",
    "        audio = super()._repr_html_()\n",
    "        audio = audio.replace('<audio', f'<audio onended=\"this.parentNode.removeChild(this)\"')\n",
    "        return f'<div style=\"display:none\">{audio}</div>'\n",
    "\n",
    "class Beeper:\n",
    "\n",
    "    def __init__(self, threshold, **audio_kwargs):\n",
    "        self.threshold = threshold\n",
    "        self.start_time = None    # time in sec, or None\n",
    "        self.audio = audio_kwargs\n",
    "\n",
    "    def pre_execute(self):\n",
    "        if not self.start_time:\n",
    "            self.start_time = time()\n",
    "\n",
    "    def post_execute(self):\n",
    "        end_time = time()\n",
    "        if self.start_time and end_time - self.start_time > self.threshold:\n",
    "            audio = InvisibleAudio(**self.audio, autoplay=True)\n",
    "            display(audio)\n",
    "        self.start_time = None\n",
    "\n",
    "ipython = get_ipython()        \n",
    "        \n",
    "try:\n",
    "    del beeper\n",
    "    ipython.events.unregister('pre_execute', beeper.pre_execute)\n",
    "    ipython.events.unregister('post_execute', beeper.post_execute)\n",
    "except Exception as e:\n",
    "    print(\"nothing to unregister \", str(e))\n",
    "    \n",
    "beeper = Beeper(5, url='http://www.soundjay.com/button/beep-07.wav')\n",
    "\n",
    "ipython.events.register('pre_execute', beeper.pre_execute)\n",
    "ipython.events.register('post_execute', beeper.post_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class DataManager:\n",
    "    \n",
    "    root_dir_ = '.'\n",
    "    \n",
    "    UNKNOWN_WORD = \"<UNK>\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, root_dir='.', unknown_word='<UNK>'):\n",
    "        self.root_dir_ = root_dir\n",
    "        self.UNKNOWN_WORD = unknown_word\n",
    "        \n",
    "    def load_dummy_data(self):\n",
    "        \"\"\"\n",
    "        This method makes available some dummy training data.\n",
    "        \"\"\"\n",
    "        X = self.pickle_load(os.path.join('data', '/fr_X.pkl'))\n",
    "        Y = self.pickle_load(\"data/fr_Y.pkl\")\n",
    "        vocab_mots = self.pickle_load(\"data/fr_vocab_mots.pkl\")\n",
    "        vocab_pdd = self.pickle_load(\"data/fr_vocab_pdd.pkl\")\n",
    "        vocab_liaisons = self.pickle_load(\"data/fr_vocab_liaisons.pkl\")\n",
    "        return X, Y, vocab_mots, vocab_pdd, vocab_liaisons\n",
    "        \n",
    "    def load_dummy_data_2(self):\n",
    "        \"\"\"\n",
    "        This method makes available some dummy training data.\n",
    "        \"\"\"\n",
    "        data = self.pickle_load(\"data/f1_fr_project_ok_bool.pkl\")\n",
    "        return data['X'], data['Y'], data['vocab_mots'], data['vocab_pdd'], data['vocab_liaisons'] \n",
    "    \n",
    "    def load_data(self, phase='train', lang='fr', featureset='f1'):\n",
    "        \"\"\"\n",
    "        Loads a dataset for a specific lang and feature set, and phase (train/dev/test).\n",
    "        Note: if unknown word is not present in WORDS or LEMMA vocab, then it is appended.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        phase: str\n",
    "            'train', 'dev' or 'test'\n",
    "            \n",
    "        lang: str\n",
    "        \n",
    "        featureset: str\n",
    "            'f1', 'f2' or 'f3'\n",
    "        \n",
    "        \"\"\"\n",
    "        name = \"{featureset}_{lang}-{phase}\".format(lang=lang, featureset=featureset, phase=phase)\n",
    "        fname = os.path.join(self.root_dir_, 'data', name)\n",
    "        data = self.pickle_load(fname)\n",
    "        if data:\n",
    "            vocabs = {}\n",
    "            X = np.array(data['X'])\n",
    "            Y = np.array(data['Y'])\n",
    "            vocabs['WORDS'] = data['vocab_mots']\n",
    "            vocabs['POS'] = data['vocab_pdd']\n",
    "            vocabs['LABELS'] = data['vocab_liaisons']\n",
    "            if isinstance(vocabs['WORDS'], np.ndarray):\n",
    "                vocabs['WORDS'] = vocabs['WORDS'].tolist()\n",
    "            if self.UNKNOWN_WORD not in vocabs['WORDS']:\n",
    "                vocabs['WORDS'].append(self.UNKNOWN_WORD)\n",
    "                \n",
    "            if isinstance(vocabs['POS'], np.ndarray):\n",
    "                vocabs['POS'] = vocabs['POS'].tolist()\n",
    "            if isinstance(vocabs['LABELS'], np.ndarray):\n",
    "                vocabs['LABELS'] = vocabs['LABELS'].tolist()\n",
    "            if self.UNKNOWN_WORD not in vocabs['LABELS']:\n",
    "                vocabs['LABELS'].append(self.UNKNOWN_WORD)\n",
    "                \n",
    "            if featureset == 'f2' or featureset == 'f3':\n",
    "                vocabs['MORPHO'] = data['vocab_morpho']\n",
    "                vocabs['LEMMA'] = data['vocab_lemma']\n",
    "                if isinstance(vocabs['MORPHO'], np.ndarray):\n",
    "                    vocabs['MORPHO'] = vocabs['MORPHO'].tolist()\n",
    "                if self.UNKNOWN_WORD not in vocabs['MORPHO']:\n",
    "                    vocabs['MORPHO'].append(self.UNKNOWN_WORD)\n",
    "                if isinstance(vocabs['LEMMA'], np.ndarray):\n",
    "                    vocabs['LEMMA'] = vocabs['LEMMA'].tolist()\n",
    "                if self.UNKNOWN_WORD not in vocabs['LEMMA']:\n",
    "                    vocabs['LEMMA'].append(self.UNKNOWN_WORD)\n",
    "                          \n",
    "            print(\"load_data: loaded X = \", X.shape, \", Y = \", Y.shape, \", vocabs = \", \n",
    "                  (''.join(\"{key} ({len}), \".format(key=k, len=len(vocabs[k])) for k in vocabs.keys())))\n",
    "            return X, Y, vocabs\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def merge_vocabs(self, vocab1, vocab2, data, columns, test_mode=False, verbose=False):\n",
    "        \"\"\"\n",
    "        Merges vocab2 into vocab1, updating accordingly words references in data columns.\n",
    "        For all words in vocab2 with index idx_vocab2:\n",
    "        - if it exists in vocab1, then words features of data in columns refering to idx_vocab2 will be replaced by idx_vocab1\n",
    "        - if it does not exist in vocab1, \n",
    "          - If test_mode is False, then it will be appended to vocab1 then replacement will be done as above in data\n",
    "          - If test_mode is True, then pointers to idx_vocab2 in data columns, will be replaced by pointers to unknown word\n",
    "            from vocab1 (this may be used to align a test vocab/data to a neural network vocab, as we are not supposed\n",
    "            to change then neural network vocab once trained on train/dev data)\n",
    "        When test_mode is True, vocab1 is left unchanged.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        vocab1: list(str)\n",
    "            A list of words from original vocabulary.\n",
    "                        \n",
    "        vocab2: list(str)\n",
    "            A list of words from vocabulary to be merged into vocab1 (used to build X2).\n",
    "            \n",
    "        data: array\n",
    "            Array containing the words indices to be updated by merge of vocabularies.\n",
    "            Usually this should be an array with samples as first dimension (rows), then columns for features.\n",
    "\n",
    "        columns: tuple(int)\n",
    "            Indices of columns to be updated in data.\n",
    "            \n",
    "        test_mode: boolean\n",
    "            \n",
    "        Returns:\n",
    "        \n",
    "        vocab1: list(str)\n",
    "            A new vocabulary with missing words appended to vocab1.\n",
    "            Note: vocab1 is also update in place, meaning this function modified original vocab1.\n",
    "        \n",
    "        ! data is updated in place.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        vocab1_ = vocab1.copy()\n",
    "        data_ = data.copy()\n",
    "        \n",
    "        unknown_idx = -1\n",
    "        if self.UNKNOWN_WORD in vocab1:\n",
    "            unknown_idx = vocab1.index(self.UNKNOWN_WORD)\n",
    "        \n",
    "        for idx_vocab2, w in enumerate(vocab2):\n",
    "            # treat vocabs\n",
    "            if w in vocab1:\n",
    "                if verbose:\n",
    "                    print(\"word [{i}]={w} found in vocab1 at {j}\".format(i=idx_vocab2, w=w, j=vocab1.index(w)))\n",
    "                idx_vocab1 = vocab1.index(w)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"word [{i}]={w} not found in vocab1\".format(i=idx_vocab2, w=w))\n",
    "                if not test_mode:\n",
    "                    vocab1.append(w)\n",
    "                    idx_vocab1 = len(vocab1) - 1\n",
    "                elif unknown_idx is not -1:\n",
    "                    # if in test mode, we associate missing word to \"unknown word\"\n",
    "                    idx_vocab1 = unknown_idx\n",
    "                else:\n",
    "                    print(\"merge_vocabs: word [{i}]={w} not found in vocab1 but no unknown word '{unk}' defined in vocab1\"\n",
    "                         .format(i=idx_vocab2, w=w, unk=self.UNKNOWN_WORD))\n",
    "                \n",
    "            # replace all references in data\n",
    "            for i in range(len(data_)):\n",
    "                if len(data_.shape) == 1:\n",
    "                    if data_[i] == idx_vocab2:\n",
    "                        if verbose:\n",
    "                            print(\"Replacing word [{i}]= {idx}, {w} index {fro} to {to}\"\n",
    "                                    .format(i=i, idx=idx_vocab2, w=vocab2[idx_vocab2], \n",
    "                                            fro=idx_vocab2, to=idx_vocab1))\n",
    "                        data[i] = idx_vocab1\n",
    "                        #Replacing word [796]= 1, LEFT_advcl index 1 to 2\n",
    "                        \n",
    "                else:\n",
    "                    for idx, j in enumerate(columns):\n",
    "                        if data_[i][j] == idx_vocab2:\n",
    "                            if verbose:\n",
    "                                print(\"Replacing word [{i},{j}]= {idx}, {w} index {fro} to {to}\"\n",
    "                                      .format(i=i, j=j, idx=idx_vocab2, w=vocab2[idx_vocab2], \n",
    "                                              fro=idx_vocab2, to=idx_vocab1))\n",
    "                            data[i][j] = idx_vocab1\n",
    "\n",
    "        return vocab1, data\n",
    "        \n",
    "\n",
    "    # some utilities for saving results\n",
    "    def safe_pickle_dump(self, filename, obj):\n",
    "        \"\"\"\n",
    "        Serializes an object to file, creating directory structure if it does not exist.\n",
    "        \"\"\"\n",
    "        name = filename\n",
    "        print(\"pickle.dump \"+name)\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(name), exist_ok=True)\n",
    "            f = open(name,\"wb\")\n",
    "            pickle.dump(obj,f)\n",
    "            f.close()\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "        return True\n",
    "    \n",
    "    def pickle_load(self, filename):\n",
    "        \"\"\"\n",
    "        Deserialize an object from a file created with pickle.dump.\n",
    "        Returns False if this failed.\n",
    "        \"\"\"\n",
    "        name = filename\n",
    "        print(\"pickle.load \"+name)\n",
    "        try:\n",
    "            f = open(name,\"rb\")\n",
    "            obj = pickle.load(f)\n",
    "            f.close()\n",
    "            return obj\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "      \n",
    "        return None\n",
    "        \n",
    "    def load_embeddings(self, lang, type='fasttext'):\n",
    "        \"\"\"\n",
    "        Loads an embeddings file depending on its type and language.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        type: str\n",
    "            Only \"fasttext\" is supported.\n",
    "            \n",
    "        lang: str\n",
    "            See load_fasttext_embeddings(lang)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.load_fasttext_embeddings(lang)\n",
    "        \n",
    "    def load_fasttext_embeddings(self, lang):\n",
    "        \"\"\"\n",
    "        Loads a fasttext embedding, chosen depending on lang parameter provided.\n",
    "        File expected as root_dir_/data/embeddings/facebookresearch/wiki.{lang}.vec\n",
    "        (or as root_dir_/cache/wiki.{lang}.vec.pkl if already loaded once)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        lang: str\n",
    "            One of 'fr', 'ja', 'en', 'nl' (or additional ones depending on embeddings present on disk).\n",
    "        \n",
    "        \"\"\"\n",
    "        data_dict = {}\n",
    "        apax = \"\"\n",
    "        \n",
    "        pickle_fname = \"wiki.{lang}.vec.pkl\".format(lang=lang)\n",
    "        pickle_ffname = os.path.join(self.root_dir_, 'cache', pickle_fname)\n",
    "             \n",
    "        if os.path.isfile(pickle_ffname):\n",
    "            data_dict = self.pickle_load(pickle_ffname)\n",
    "            print(\"Embedding for {lang} loaded from {fname}\".format(lang=lang, fname=pickle_ffname))\n",
    "        else: \n",
    "            fname = \"wiki.{lang}.vec\".format(lang=lang)\n",
    "            data_file = os.path.join(self.root_dir_, 'data', 'embeddings', 'facebookresearch', fname)\n",
    "        \n",
    "            fin = io.open(data_file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "            n, d = map(int, fin.readline().split())\n",
    "            \n",
    "            for line in fin:\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                data_dict[tokens[0]] = list(map(float, tokens[1:]))\n",
    "            print(\"Embedding for {lang} loaded from {fname}\".format(lang=lang, fname=data_file))\n",
    "            # save embeddings as array format to improve speed next time\n",
    "            self.safe_pickle_dump(pickle_ffname, data_dict)\n",
    "        \n",
    "        # adds unknown word if required, using embedding of apax word\n",
    "        if self.UNKNOWN_WORD not in data_dict.keys():\n",
    "            apax = list(data_dict.keys())[-1] # python does not guarantee order of dict keys but normally should be ok\n",
    "            print(\"debug apax \", apax)\n",
    "            data_dict[self.UNKNOWN_WORD] = data_dict[apax]\n",
    "\n",
    "        print(\"load_fasttext_embeddings: loaded \", len(data_dict), \" words vectors, apax '\", apax, \"'\")\n",
    "        return data_dict, apax\n",
    "    \n",
    "    def get_words_to_match_for_embeddings(self, word):\n",
    "        \"\"\"\n",
    "        Returns a list with same word, word lowercased, word lowercased and dashes removed, \n",
    "        then all this plus \\xa0 removed.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        word: str\n",
    "            Word to be transformed.\n",
    "            \n",
    "        Returns: list\n",
    "            List of transformed word forms.\n",
    "        \"\"\"\n",
    "        return [word, word.lower(), word.lower().replace('-', ''), word.lower().replace('-', '').replace('\\\\xa0', ' ')]\n",
    "    \n",
    "    def align_embeddings(self, vocab, embeddings, augment_vocab=True, max_vocab_size=-1):\n",
    "        \"\"\"\n",
    "        Generates aligned embeddings from original embeddings, and a vocabulary of words.\n",
    "        Words from vocabulary may not exist in original embeddings, in this case a random vector is generated.\n",
    "        Words matching is done as (by priority) : exact match, then case insensitively, then with dash ('-') removed.\n",
    "        Exception: unknown word is matched with lowest frequency word from embeddings corpus (with fasttext, it is the last\n",
    "        one from the list).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        vocab: array\n",
    "            An array containing each word in the vocabulary.\n",
    "            \n",
    "        embeddings: dict\n",
    "            A dict object with words as keys and their embeddings (as a vector array) as values.\n",
    "            \n",
    "        augment_vocab: boolean\n",
    "            If True, then all words from embeddings not existing in vocab, are appended to vocab (up to max_vocab_size).\n",
    "            \n",
    "        max_vocab_size: int\n",
    "            Maximum length of resulting vocab if augment_vocab is True and if max_vocab_size < original vocab length.\n",
    "            If -1 then there is no limit.\n",
    "            Note: resulting vocab size can't be < original vocab size, whatever the value of max_vocab_size.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        aligned_embeddings: list\n",
    "            An array containing:\n",
    "          - for words from vocab found in embeddings, the corresponding embedding at same index as in vocab.\n",
    "          - for words not found in embeddings, a random vector, at same index as in vocab.\n",
    "          - if augment_vocab is True, all remaining words from embeddings (not found in vocab) are added after\n",
    "            len(vocab)\n",
    "        \n",
    "        words_not_found: list\n",
    "            An array containing indices (based on vocab) of words not found in embeddings and replaced by random\n",
    "            values.\n",
    "            \n",
    "        words_matched: list\n",
    "            An array of strings of words based on vocab, as they were matched in embeddings.\n",
    "            For example if lowercased word from vocab was matched, then lowercase version of this word will be found\n",
    "            in this table (whereas the original case sensitive word will remain as is in vocab array)\n",
    "        \n",
    "        \"\"\"\n",
    "        dim_embedding = len(embeddings[list(embeddings.keys())[0]]) # find length from value of 'first' key\n",
    "        \n",
    "        print(\"align_embeddings: aligning embeddings ({elen},{edim}) with vocab ({vlen}) using at most {max}\"\n",
    "             .format(elen=len(embeddings), edim=dim_embedding, vlen=len(vocab), max=max_vocab_size))\n",
    "        \n",
    "        cur_size = len(vocab) # to avoid computing vocab len at each iteration\n",
    "        \n",
    "        # first append missing embeddings to vocab, if required, to limit unknown words\n",
    "        if augment_vocab:\n",
    "            for embedding_word in embeddings.keys():\n",
    "                # adds missing word only up to max vocab size, if used\n",
    "                if max_vocab_size is not -1 and cur_size > max_vocab_size:\n",
    "                    break\n",
    "                elif embedding_word not in vocab:\n",
    "                    vocab.append(embedding_word)\n",
    "                    cur_size = cur_size + 1\n",
    "#                    if cur_size % 100 == 0:\n",
    "#                        print(\"debug embed cur_size \", cur_size)\n",
    "        \n",
    "            print(\"align_embeddings: new augmented vocab size : \", len(vocab))\n",
    "        \n",
    "        aligned_embeddings = np.zeros((len(vocab), dim_embedding))\n",
    "        words_not_found = []\n",
    "        words_matched = [None] * len(vocab)\n",
    "        for idx_mot, mot in enumerate(vocab):\n",
    "            words_to_match = self.get_words_to_match_for_embeddings(mot)\n",
    "            for word_to_match in words_to_match:\n",
    "                if word_to_match in embeddings.keys():\n",
    "                    aligned_embeddings[idx_mot] = embeddings[word_to_match]\n",
    "                    words_matched[idx_mot] = word_to_match\n",
    "                    break\n",
    "            if words_matched[idx_mot] is None:\n",
    "                words_not_found.append(idx_mot)\n",
    "                words_matched[idx_mot] = mot\n",
    "                aligned_embeddings[idx_mot] = np.random.rand(dim_embedding)\n",
    "        \n",
    "        print(\"align_embeddings: new embeddings shape {shap}, words not found {wnf}, words found {wf}\"\n",
    "             .format(shap=aligned_embeddings.shape, wnf=len(words_not_found), wf=len(words_matched)))\n",
    "        return aligned_embeddings, words_not_found, words_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation, Concatenate, Embedding, concatenate, Flatten, Dropout\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DependencyClassifier:\n",
    "    \n",
    "    UNKNOWN_WORD = '<UNK>' \n",
    "    \n",
    "    MAX_VOCAB_SIZE = -1\n",
    "    \n",
    "    models_ = {}\n",
    "    networks_ = {}\n",
    "    path_ = '.'\n",
    "    \n",
    "    current_model_ = None\n",
    "    current_network_ = None\n",
    "    \n",
    "    embeddings_ = None\n",
    "    embeddings_for_words_ = None\n",
    "    embeddings_for_lemmas_ = None\n",
    "    \n",
    "    dm_ = DataManager()\n",
    "    \n",
    "    def __init__(self, path='.', max_vocab_size=-1, unknown_word='<UNK>'):\n",
    "        \"\"\"\n",
    "        Class to handle neural networks for TAL - TBP AE purpose.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        path: str\n",
    "            Root path to find files (path/.), scripts (path/.), cache (path/cache), embeddings (path/data/embeddings/...) ...\n",
    "            Current path by default.\n",
    "            \n",
    "        max_vocab_size: int\n",
    "            Maximum length of a vocabulary - hence for an embedding matrix used in a network.\n",
    "            This can be set to limit amount of memory used by vocabs and embeddings.\n",
    "            In practice, original pre-trained embeddings vectors are truncated up to this length, considering\n",
    "            that embeddings used (fasttext) are ordered from most frequent to least frequent word.\n",
    "            -1 (default value) means no limitation.\n",
    "        \n",
    "        \"\"\"\n",
    "        if path != '.':\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.path_ = path\n",
    "        self.MAX_VOCAB_SIZE = max_vocab_size\n",
    "        self.UNKNOWN_WORD = '<UNK>'\n",
    "        self.dm_ = DataManager(self.path_)\n",
    "        \n",
    "    def get_model(self, model_name):\n",
    "        \"\"\"\n",
    "        Returns an existing TAL model, or None.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            Name of the model.\n",
    "        \"\"\"\n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        return self.models_[model_name]\n",
    "    \n",
    "    def get_current_model(self):\n",
    "        return self.models_[self.current_model]\n",
    "        \n",
    "    def create_model(self, model_name, lang, featureset, use_forms, vocabs, vocabs_dev, vocabs_test, \n",
    "                     status=False, test_status=False, embeddings_file=None):\n",
    "        \"\"\"\n",
    "        Creates a new TAL model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            Name of this model.\n",
    "            \n",
    "        lang: str\n",
    "            'fr', 'ja', 'nl', 'en'\n",
    "            \n",
    "        featureset: str\n",
    "            'f1', 'f2' or 'f3'\n",
    "            \n",
    "        use_forms: boolean\n",
    "            If true then both words forms are added on top of the features.\n",
    "            \n",
    "        vocabs, vocabs_dev, vocabs_test: dict\n",
    "            Vocabs for learning task, with keys 'WORDS', 'POS', 'MORPHO' and/or 'LEMMA' (or 'LABELS' for targets).\n",
    "        \n",
    "        status: boolean\n",
    "            If a network was prepared, created and trained for this model.\n",
    "            \n",
    "        test_status: boolean\n",
    "            If test results were produced for this model (ie conllu test file)\n",
    "        \n",
    "        \"\"\"\n",
    "        model = {'name': model_name,\n",
    "                'lang': lang,\n",
    "                'featureset': featureset,\n",
    "                'use_forms': use_forms,\n",
    "                'vocabs': vocabs,\n",
    "                'vocabs_dev': vocabs_dev,\n",
    "                'vocabs_test': vocabs_test,\n",
    "                'status': status,\n",
    "                'test_status': test_status,\n",
    "                'embeddings_file': embeddings_file}\n",
    "        if model_name not in self.models_.keys():\n",
    "            self.models_[model_name] = model\n",
    "        \n",
    "        self.current_model_ = model_name\n",
    "        \n",
    "        return model\n",
    "            \n",
    "    def remove_model(self, model_name):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if model_name in self.models_.keys():\n",
    "            if self.current_model_ == model_name:\n",
    "                self.current_model_ = None\n",
    "            return self.models_.pop(model_name, None)\n",
    "    \n",
    "    def save_model(self, model_name):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if model_name in self.models_:\n",
    "            self.dm_.safe_pickle_dump(os.path.join(self.path_, model_name + '-model.pkl'), self.models_[model_name])\n",
    "        else:\n",
    "            print(\"Model \" + model_name + \" not found\")\n",
    "            \n",
    "    def load_model(self, model_name):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        model_conf = self.dm_.pickle_load(os.path.join(self.path_, model_name + '-model.pkl'))\n",
    "        if model_conf is not None:\n",
    "            self.create_model(model_name=model_conf['name'], lang=model_conf['lang'], featureset=model_conf['featureset'], \n",
    "                              vocabs=model_conf['vocabs'], vocabs_dev=model_conf['vocabs_dev'],\n",
    "                              vocabs_test=model_conf['vocabs_test'], use_forms=model_conf['use_forms'], \n",
    "                              status=model_conf['status'], test_status=model_conf['test_status'])\n",
    "            self.current_model_ = model_name\n",
    "            return True\n",
    "        return False\n",
    "            \n",
    "    def set_model_done(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            self.models_[model_name]['status'] = True\n",
    "        \n",
    "    def set_model_undone(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            self.models_[model_name]['status'] = True\n",
    "            \n",
    "    def get_model_status(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            return self.models_[model_name]['status']\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def set_model_test_done(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            self.models_[model_name]['test_status'] = True\n",
    "        \n",
    "    def set_model_test_undone(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            self.models_[model_name]['test_status'] = True\n",
    "            \n",
    "    def get_model_test_status(self, model_name):\n",
    "        if model_name in self.models_:\n",
    "            return self.models_[model_name]['test_status']\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def create_network(self, network_name, model_name, nb_classes, dropout=False, hidden_dim=200):\n",
    "        \"\"\"\n",
    "        Creates a keras architecture appropriate for a TAL model.\n",
    "        Note: a number of parameters comes from the TAL model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        nb_classes: int\n",
    "            Number of classes recognized by the network.\n",
    "            \n",
    "        dropout: boolean\n",
    "            Whether to add dropout layers or not.\n",
    "            \n",
    "        hidden_dim: int\n",
    "            The size of the hidden layers.\n",
    "            (network will consist of a first layer of hidden_dim and a second layer of hidden_dim/2 neurons)\n",
    "            \n",
    "        \"\"\"\n",
    "        net_model = None\n",
    "        \n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        \n",
    "        model = self.models_[model_name]\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model['use_forms']\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        \n",
    "        net_model = Model()\n",
    "                       \n",
    "        #if embeddings != None:\n",
    "            \n",
    "        embedsw = self.embeddings_for_words_\n",
    "        embedsl = self.embeddings_for_lemmas_\n",
    "        \n",
    "        if embedsw is not None:\n",
    "            dim_embeddings = embedsw.shape[1]\n",
    "        else:\n",
    "            dim_embeddings = 300\n",
    "        \n",
    "        concat_layers = []\n",
    "        input_layers = []\n",
    "        \n",
    "        if use_forms:\n",
    "            if embedsw is not None:\n",
    "            \n",
    "                if len(vocabs['WORDS']) != embedsw.shape[0]:\n",
    "                    print(\"Words vocab size {v} must equal embeddings length {e}\".format(v=len(vocabs['WORDS']), \n",
    "                                                                               e=embedsw.shape[0]))\n",
    "                    return None\n",
    "            \n",
    "                # Pretrained Embedding layer for 2 words\n",
    "                input_word1 = Input(shape=(1,), dtype='int32', name='word_input_1')\n",
    "                embeddings_w1 = Embedding(\n",
    "                    input_dim=len(vocabs['WORDS']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    weights=[embedsw], \n",
    "                    input_length=1)(input_word1)\n",
    "                embeddings_w1 = Flatten()(embeddings_w1)\n",
    "                \n",
    "                # Embedding layer for second word\n",
    "                input_word2 = Input(shape=(1,), dtype='int32', name='word_input_2')\n",
    "                embeddings_w2 = Embedding(input_dim=len(vocabs['WORDS']), \n",
    "                                     output_dim=dim_embeddings, \n",
    "                                     weights=[embedsw], \n",
    "                                     input_length=1)(input_word2)\n",
    "                #embeddings_2 = embeddings_layer(input_word2) # sharing weights between both words embeddings\n",
    "                embeddings_w2 = Flatten()(embeddings_w2)\n",
    "            \n",
    "            else:\n",
    "                # Embedding layer for 2 words\n",
    "                input_word1 = Input(shape=(1,), dtype='int32', name='word_input_1')\n",
    "                embeddings_w1 = Embedding(\n",
    "                    input_dim=len(vocabs['WORDS']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    input_length=1)(input_word1)\n",
    "                embeddings_w1 = Flatten()(embeddings_w1)\n",
    "                \n",
    "                # Embedding layer for second word\n",
    "                input_word2 = Input(shape=(1,), dtype='int32', name='word_input_2')\n",
    "                embeddings_w2 = Embedding(\n",
    "                    input_dim=len(vocabs['WORDS']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    input_length=1)(input_word2)\n",
    "                embeddings_w2 = Flatten()(embeddings_w2)\n",
    "                \n",
    "            concat_layers.append(embeddings_w1)\n",
    "            concat_layers.append(embeddings_w2)\n",
    "            input_layers.append(input_word1)\n",
    "            input_layers.append(input_word2)\n",
    "            \n",
    "        if 'LEMMA' in vocabs.keys():\n",
    "            if featureset is not 'f1' and embedsl is not None:\n",
    "                # we must define also inputs and embeddings for lemmas\n",
    "                if len(vocabs['LEMMA']) != embedsl.shape[0]:\n",
    "                    print(\"Lemma vocab size {v} must equal embeddings length {e}\".format(v=len(vocabs['LEMMA']), \n",
    "                                                                                   e=embedsl.shape[0]))\n",
    "                    return None\n",
    "            \n",
    "                # Pretrained Embedding layer for 2 lemmas\n",
    "                input_lemma1 = Input(shape=(1,), dtype='int32', name='lemma_input_1')\n",
    "                embeddings_l1 = Embedding(\n",
    "                    input_dim=len(vocabs['LEMMA']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    weights=[embedsl], \n",
    "                    input_length=1)(input_lemma1)\n",
    "                embeddings_l1 = Flatten()(embeddings_l1)\n",
    "                \n",
    "                # Embedding layer for second word\n",
    "                input_lemma2 = Input(shape=(1,), dtype='int32', name='lemma_input_2')\n",
    "                embeddings_l2 = Embedding(\n",
    "                    input_dim=len(vocabs['LEMMA']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    weights=[embedsl], \n",
    "                    input_length=1)(input_lemma2)\n",
    "                embeddings_l2 = Flatten()(embeddings_l2)\n",
    "            \n",
    "                concat_layers.append(embeddings_l1)\n",
    "                concat_layers.append(embeddings_l2)\n",
    "                input_layers.append(input_lemma1)\n",
    "                input_layers.append(input_lemma2)\n",
    "            \n",
    "            elif featureset is not 'f1':\n",
    "            \n",
    "                # Embedding layer for 2 lemmas\n",
    "                input_lemma1 = Input(shape=(1,), dtype='int32', name='lemma_input_1')\n",
    "                embeddings_l1 = Embedding(\n",
    "                    input_dim=len(vocabs['LEMMA']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    input_length=1)(input_lemma1)\n",
    "                embeddings_l1 = Flatten()(embeddings_l1)\n",
    "                    \n",
    "                # Embedding layer for second word\n",
    "                input_lemma2 = Input(shape=(1,), dtype='int32', name='lemma_input_2')\n",
    "                embeddings_l2 = Embedding(\n",
    "                    input_dim=len(vocabs['LEMMA']), \n",
    "                    output_dim=dim_embeddings, \n",
    "                    input_length=1)(input_lemma2)\n",
    "                embeddings_l2 = Flatten()(embeddings_l2)  \n",
    "            \n",
    "                concat_layers.append(embeddings_l1)\n",
    "                concat_layers.append(embeddings_l2)\n",
    "                input_layers.append(input_lemma1)\n",
    "                input_layers.append(input_lemma2)\n",
    "            \n",
    "        \n",
    "        # define input for additional features\n",
    "        # note: dist is restricted to [-7 ... 7] so 15 values\n",
    "        if featureset == 'f1':\n",
    "            \"\"\" S.0.POS\n",
    "                B.0.POS\n",
    "                DIST\"\"\"\n",
    "            dim_features = len(vocabs['POS']) * 2 + 15\n",
    "        elif featureset == 'f2':\n",
    "            \"\"\" S.0.POS\n",
    "                S.0.LEMMA   # embeddings\n",
    "                S.0.MORPHO\n",
    "                S.-1.POS\n",
    "                B.0.POS\n",
    "                B.0.LEMMA   # embeddings\n",
    "                B.0.MORPHO\n",
    "                B.-1.POS\n",
    "                B.1.POS\n",
    "                DIST\"\"\"\n",
    "            dim_features = len(vocabs['POS']) * 4  + len(vocabs['MORPHO']) * 2 + 15\n",
    "        else:\n",
    "            \"\"\" S.0.POS\n",
    "                S.0.LEMMA   # embeddings\n",
    "                S.0.MORPHO\n",
    "                S.-1.POS\n",
    "                B.0.POS\n",
    "                B.0.LEMMA   # embeddings\n",
    "                B.0.MORPHO\n",
    "                B.-2.POS\n",
    "                B.-1.POS\n",
    "                B.1.POS\n",
    "                DIST\"\"\"\n",
    "            # same size as 'f2'\n",
    "            dim_features = len(vocabs['POS']) * 5 + len(vocabs['MORPHO']) * 2 + 15\n",
    "        print(\"  expecting {n} features dim\".format(n=dim_features))\n",
    "        \n",
    "        # define input for features\n",
    "        features_input = Input(shape=(dim_features,))\n",
    "        concat_layers.append(features_input)\n",
    "        input_layers.append(features_input)\n",
    "        \n",
    "        # concatenate inputs if there's more than one\n",
    "        if len(concat_layers) > 1:\n",
    "            l = concatenate(concat_layers)\n",
    "        else:\n",
    "            l = features_input\n",
    "            \n",
    "        #if embeddings is not None:\n",
    "            # concatenate features and embeddings\n",
    "            #l = concatenate([embeddings_1, embeddings_2, features_input])\n",
    "        #else:\n",
    "            #l = features_input\n",
    "        \n",
    "        # adding dense layers\n",
    "        \n",
    "        l = Dense(hidden_dim)(l)\n",
    "        l = Activation('relu')(l)\n",
    "        if dropout:\n",
    "            l = Dropout(0.15)(l)\n",
    "        l = Dense(int(hidden_dim / 2))(l)\n",
    "        l = Activation('relu')(l)\n",
    "        if dropout:\n",
    "            l = Dropout(0.15)(l)\n",
    "        \n",
    "        l = Dense(nb_classes)(l)\n",
    "        out = Activation('softmax')(l)\n",
    "        \n",
    "        if len(input_layers) > 1:\n",
    "            net_model = Model(input_layers, out)\n",
    "        else:\n",
    "            net_model = Model(features_input, out)\n",
    "        \n",
    "        # not sure where to compile the model ...\n",
    "        net_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            \n",
    "        if net_model:\n",
    "            self.networks_[network_name] = net_model\n",
    "            # save initial state of this network (this also saves embedding layers, so we remove them)\n",
    "            self.save_network(network_name)\n",
    "            #del self.embeddings_for_words_\n",
    "            #del self.embeddings_for_lemmas_\n",
    "            self.current_network_ = network_name\n",
    "        return net_model\n",
    "\n",
    "    def save_network(self, network_name):\n",
    "        if network_name in self.networks_:\n",
    "            self.networks_[network_name].save(os.path.join(self.path_, network_name + '-net.h5'))\n",
    "        else:\n",
    "            print(\"Net \" + network_name + \" not found\")\n",
    "            \n",
    "    def load_network(self, network_name):\n",
    "        \"\"\"\n",
    "        Loads a keras network model (.h5, architecture and weights) from disk.\n",
    "        \"\"\"\n",
    "        fname = os.path.join(self.path_, network_name + '-net.h5')\n",
    "        if os.path.isfile(fname):\n",
    "            try:\n",
    "                self.networks_[network_name] = load_model(fname)\n",
    "                self.current_network_ = network_name\n",
    "            except Exception as e:\n",
    "                print(\"Could not load keras model because \", str(e))\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_network(self, network_name):\n",
    "        if network_name in self.networks_:\n",
    "            return self.networks_[network_name]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_current_network(self):\n",
    "        if self.current_network_ in self.networks_:\n",
    "            return self.networks_[self.current_network_]\n",
    "\n",
    "    def remove_network(self, network_name):\n",
    "        \"\"\"\n",
    "        Removes a network model from DependencyClassifier (not from disk !), if it exists it is returned by this method.\n",
    "        \"\"\"\n",
    "        if network_name in self.networks_.keys():\n",
    "            self.current_network_ = None\n",
    "            return self.networks_.pop(model_name, None)    \n",
    "    \n",
    "    def preprocess_embeddings(self, model_name, augment_vocabs=True):\n",
    "        \"\"\"\n",
    "        Preprocess embeddings for training a network for this model.\n",
    "        Notes:\n",
    "        - should be run once BEFORE creating a network, if you want to use pre-trained embeddings to train this network, \n",
    "          if not embeddings will be completely learned during training\n",
    "        - loaded embeddings should be deleted before loading/running a new network to avoid exhausting memory.\n",
    "        - if called again for another model embeddings will be REPLACED\n",
    "        - for already created network embedded remain and will be saved along with the network (no need to call this if\n",
    "          you load a saved network)\n",
    "            - at network creation, once net model is saved to disk with keras model.save(), loaded embeddings are deleted\n",
    "              to free memory\n",
    "            - to manually remove loaded embeddings, delete attributes embeddings_, embeddings_for_words_ and embeddings_for_lemmas_\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            Name of the TAL model.\n",
    "            \n",
    "        Returns: nothing\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        model = self.models_[model_name]\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        \n",
    "        if (self.embeddings_for_words_ is None and use_forms) or self.embeddings_for_lemmas_ is None:\n",
    "        \n",
    "            if self.embeddings_ is None:\n",
    "                print(\"preprocess_embeddings: loading original embeddings ...\")\n",
    "                self.embeddings_, self.apax_ = self.dm_.load_embeddings(lang)\n",
    "            \n",
    "            faligned = os.path.join(self.path_, 'cache', model_name + '-embeddings-forms-aligned.pkl')\n",
    "            if os.path.isfile(faligned):\n",
    "                self.embeddings_for_words_ = self.dm_.pickle_load(faligned)\n",
    "            if self.embeddings_for_words_ is None:\n",
    "                print(\"preprocess_embeddings: aligning embeddings for forms ...\")\n",
    "                self.embeddings_for_words_, words_not_found, words_matched = self.dm_.align_embeddings(\n",
    "                    vocabs['WORDS'], self.embeddings_, augment_vocab=augment_vocabs, max_vocab_size=self.MAX_VOCAB_SIZE)\n",
    "                self.dm_.safe_pickle_dump(faligned, self.embeddings_for_words_)\n",
    "                    \n",
    "            \n",
    "            faligned = os.path.join(self.path_, 'cache', model_name + '-embeddings-lemmas-aligned.pkl')\n",
    "            if os.path.isfile(faligned):\n",
    "                self.embeddings_for_lemmas_ = self.dm_.pickle_load(faligned)\n",
    "            if self.embeddings_for_lemmas_ is None and 'LEMMA' in vocabs.keys():\n",
    "                print(\"preprocess_embeddings: aligning embeddings for lemmas ...\")\n",
    "                self.embeddings_for_lemmas_, words_not_found, words_matched = self.dm_.align_embeddings(\n",
    "                    vocabs['LEMMA'], self.embeddings_, augment_vocab=augment_vocabs, max_vocab_size=self.MAX_VOCAB_SIZE)\n",
    "                self.dm_.safe_pickle_dump(faligned, self.embeddings_for_lemmas_)\n",
    "       \n",
    "        # free some memory - original embeddings should be useless now\n",
    "        del self.embeddings_\n",
    "        \n",
    "        return\n",
    "    \n",
    "            \n",
    "    def preprocess_data(self, model_name, X_train, y_train, X_dev, y_dev, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Preprocess data for training a network for this model.\n",
    "        - merges vocabs (WORDS) of X_dev into X_train\n",
    "        - merges vocabs (LEMMA) of X_dev into X_train, if any\n",
    "        - merges vocabs (MORPHO) of X_dev into X_train, if any\n",
    "        - merges vocabs (labels) of Y_dev and Y_test into Y_train\n",
    "        - converts to categorical (one-hot), features of X_* (POS, MORPHO, DIST)\n",
    "        \n",
    "        Note: arrays and vocabs are updated in place, so keep a copy of originals if required.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        model_name: str\n",
    "            TAL model to use for treating data (defines features and vocabs).\n",
    "            \n",
    "        X_train, y_train, X_dev, y_dev, X_test, y_test: arrays\n",
    "            Training, validation and test data to process.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        if model_name not in self.models_.keys():\n",
    "            print(\"Model {m} not found\".format(m=model_name))\n",
    "            return None\n",
    "        model = self.models_[model_name]\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model['use_forms']\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        vocabs_dev = model['vocabs_dev']\n",
    "        vocabs_test = model['vocabs_test']\n",
    "        \n",
    "        # Treat vocabularies\n",
    "        # Note: X_test/y_test are used only to manually check relevance of the classifier with \"fake\" arc-eager\n",
    "        # but they are not used to generate test results\n",
    "        \n",
    "        # trim data that was wrongly parsed (non-projective sentences)\n",
    "        print(\"preprocess_data: removing non-projective samples ...\")\n",
    "        print(\"    before \", X_train.shape, y_train.shape)\n",
    "        X_good = X_train[:, -1] == 1\n",
    "        X_train = X_train[X_good]\n",
    "        y_train = y_train[X_good]\n",
    "        print(\"    after \", X_train.shape, y_train.shape)\n",
    "        print(\"    before \", X_dev.shape, y_dev.shape)\n",
    "        X_good = X_dev[:, -1] == 1\n",
    "        X_dev = X_dev[X_good]\n",
    "        y_dev = y_dev[X_good]\n",
    "        print(\"    after \", X_dev.shape, y_dev.shape)\n",
    "        \n",
    "        if use_forms:\n",
    "            print(\"preprocess_data: merging WORDS vocabs from dev into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['WORDS'], vocab2=vocabs_dev['WORDS'], data=X_dev, columns=(0, 1))\n",
    "            # ... vocabs_dev['WORDS'] is now useless\n",
    "            print(\"preprocess_data: aligning WORDS vocabs from test into train ...\")            \n",
    "            # handle test set (align vocabs and set unknown words)\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['WORDS'], vocab2=vocabs_test['WORDS'], data=X_test, columns=(0, 1),\n",
    "                                 test_mode=True) # !!!\n",
    "        \n",
    "        if 'POS' in vocabs:\n",
    "            nb_classes_pos = len(np.unique(vocabs['POS']))\n",
    "        if 'MORPHO' in vocabs:\n",
    "            print(\"preprocess_data: merging and aligning MORPHO vocabs from dev into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['MORPHO'], vocab2=vocabs_dev['MORPHO'], data=X_dev, columns=(4, 8))\n",
    "            # ... vocabs_dev['MORPHO'] is now useless\n",
    "            nb_classes_morpho = len(np.unique(vocabs['MORPHO']))\n",
    "            print(\"preprocess_data: aligning MORPHO vocabs from test into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['MORPHO'], vocab2=vocabs_test['MORPHO'], data=X_test, columns=(4, 8),\n",
    "                                 test_mode=True) # there should be no UNK in this vocab normally !\n",
    "                                                \n",
    "        if 'LEMMA' in vocabs:\n",
    "            print(\"preprocess_data: merging and aligning LEMMA vocabs from dev into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['LEMMA'], vocab2=vocabs_dev['LEMMA'], data=X_dev, columns=(3, 7))\n",
    "            # ... vocabs_dev['LEMMA'] is now useless            \n",
    "            print(\"preprocess_data: aligning LEMMA vocabs from test into train ...\")\n",
    "            self.dm_.merge_vocabs(vocab1=vocabs['LEMMA'], vocab2=vocabs_test['LEMMA'], data=X_test, columns=(3, 7),\n",
    "                                 test_mode=True) # !!!\n",
    "\n",
    "        print(\"preprocess_data: merging and aligning LABEL vocabs from dev into train ...\")\n",
    "        self.dm_.merge_vocabs(vocab1=vocabs['LABELS'], vocab2=vocabs_dev['LABELS'], data=y_dev, columns=(0,))\n",
    "        print(\"preprocess_data: merging and aligning LABEL vocabs from test into train ...\")\n",
    "        self.dm_.merge_vocabs(vocab1=vocabs['LABELS'], vocab2=vocabs_test['LABELS'], data=y_test, columns=(0,),\n",
    "                             test_mode=True) # there should be no UNK in this vocab !\n",
    "        nb_classes = len(vocabs['LABELS'])\n",
    "            \n",
    "        # convert some features to one-hot encoding\n",
    "        \n",
    "        nb_classes_dist = 8\n",
    "        \n",
    "        \n",
    "        print(\"preprocess_data: converting {f} features to one-hot encoding...\".format(f=featureset))\n",
    "        \n",
    "        if featureset == 'f1':\n",
    "            \"\"\"\n",
    "            2 S.0.POS\n",
    "            3 B.0.POS\n",
    "            4 DIST\"\"\"\n",
    "            cats_pos1 = to_categorical(X_train[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(X_train[:, 3], num_classes=nb_classes_pos)\n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_train[:, 4]), num_classes=nb_classes_dist)\n",
    "            X_train = np.column_stack((X_train[:, 0], X_train[:, 1], cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "            cats_pos1 = to_categorical(X_dev[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(X_dev[:, 3], num_classes=nb_classes_pos)\n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_dev[:, 4]), num_classes=nb_classes_dist)\n",
    "            X_dev = np.column_stack((X_dev[:, 0], X_dev[:, 1], cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "            cats_pos1 = to_categorical(X_test[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(X_test[:, 3], num_classes=nb_classes_pos)\n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_test[:, 4]), num_classes=nb_classes_dist)\n",
    "            X_test = np.column_stack((X_test[:, 0], X_test[:, 1], cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "        elif featureset == 'f2': # 'f2' and 'f3' featuresets have same structure\n",
    "            \"\"\" \n",
    "            2 S.0.POS\n",
    "            3 S.0.LEMMA\n",
    "            4 S.0.MORPHO\n",
    "            5 S.-1.POS (f2, or S.1.POS for f3)\n",
    "            6 B.0.POS\n",
    "            7 B.0.LEMMA\n",
    "            8 B.0.MORPHO\n",
    "            9 B.-1.POS\n",
    "            10 B.1.POS\n",
    "            11 DIST\"\"\"                \n",
    "            cats_pos1    = to_categorical(X_train[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_train[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_train[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_train[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_train[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_train[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_train[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_train[:, 11]), num_classes=nb_classes_dist)\n",
    "            X_train = np.column_stack((X_train[:, 0], X_train[:, 1], cats_pos1, X_train[:, 3],\n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               X_train[:, 7], cat_trains_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "            \n",
    "            cats_pos1    = to_categorical(X_dev[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_dev[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_dev[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_dev[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_dev[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_dev[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_dev[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_dev[:, 11]), num_classes=nb_classes_dist)\n",
    "            X_dev = np.column_stack((X_dev[:, 0], X_dev[:, 1], cats_pos1, X_dev[:, 3],\n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               X_dev[:, 7], cat_trains_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "            \n",
    "\n",
    "            cats_pos1    = to_categorical(X_test[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_test[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_test[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_test[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_test[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_test[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_test[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_test[:, 11]), num_classes=nb_classes_dist)\n",
    "            X_test = np.column_stack((X_test[:, 0], X_test[:, 1], cats_pos1, X_test[:, 3],\n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               X_test[:, 7], cats_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "            \n",
    "        elif featureset == 'f3':\n",
    "            \"\"\" \n",
    "            2 S.0.POS\n",
    "            3 S.0.LEMMA\n",
    "            4 S.0.MORPHO\n",
    "            5 S.-1.POS    (S.1.POS does not exist)\n",
    "            6 B.0.POS\n",
    "            7 B.0.LEMMA\n",
    "            8 B.0.MORPHO\n",
    "            9 B.-2.POS    (added for f3)\n",
    "            10 B.-1.POS\n",
    "            11 B.1.POS\n",
    "            12 DIST\"\"\"                \n",
    "            cats_pos1    = to_categorical(X_train[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_train[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_train[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_train[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_train[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_train[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_train[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_pos6    = to_categorical(X_train[:, 11], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_train[:, 12]), num_classes=nb_classes_dist)\n",
    "            X_train = np.column_stack((X_train[:, 0], X_train[:, 1], cats_pos1, X_train[:, 3], cats_morpho1, \n",
    "                                       cats_pos2, cats_pos3, X_train[:, 7], cat_trains_morpho2, cats_pos4, \n",
    "                                       cats_pos5, cats_pos6, cats_dist))\n",
    "            \n",
    "            cats_pos1    = to_categorical(X_dev[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_dev[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_dev[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_dev[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_dev[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_dev[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_dev[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_pos6    = to_categorical(X_dev[:, 11], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_dev[:, 12]), num_classes=nb_classes_dist)\n",
    "            X_dev = np.column_stack((X_dev[:, 0], X_dev[:, 1], cats_pos1, X_dev[:, 3],\n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               X_dev[:, 7], cat_trains_morpho2, cats_pos4, cats_pos5, cats_pos6, cats_dist))\n",
    "            \n",
    "\n",
    "            cats_pos1    = to_categorical(X_test[:, 2], num_classes=nb_classes_pos)\n",
    "            cats_morpho1 = to_categorical(X_test[:, 4], num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(X_test[:, 5], num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(X_test[:, 6], num_classes=nb_classes_pos)\n",
    "            cats_morpho2 = to_categorical(X_test[:, 8], num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(X_test[:, 9], num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(X_test[:, 10], num_classes=nb_classes_pos)\n",
    "            cats_pos6    = to_categorical(X_test[:, 11], num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_test[:, 12]), num_classes=nb_classes_dist)\n",
    "            X_test = np.column_stack((X_test[:, 0], X_test[:, 1], cats_pos1, X_test[:, 3],\n",
    "                                      cats_morpho1, cats_pos2, cats_pos3,\n",
    "                               X_test[:, 7], cats_morpho2, cats_pos4, cats_pos5, cats_pos6, cats_dist))            \n",
    "                        \n",
    "        print(\"preprocess_data: converting LABELs to one-hot encoding ...\")\n",
    "        y_train = to_categorical(y_train, num_classes=nb_classes)\n",
    "        y_dev = to_categorical(y_dev, num_classes=nb_classes)\n",
    "        y_test = to_categorical(y_test, num_classes=nb_classes)\n",
    "                \n",
    "        return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def process_test_data(X_test):\n",
    "        \"\"\"\n",
    "        Process one sample data in order to fit it to the keras model.\n",
    "        Format should be a vector with same list of items (POS, MORPHO, LEMMA, optionnally FORM ...)\n",
    "        If there are additional columns in input they will not be used (nor returned).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        X_test: array/list\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        X_test: array/list\n",
    "            \n",
    "        vocab: list\n",
    "            The vocabulary of 'LABELS' (ie deprel, dependencies, relations ...) that can be used to interpret\n",
    "            the neural network outputs.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        model = self.get_current_model()\n",
    "        featureset = model['featureset']\n",
    "        use_forms = model['use_forms']\n",
    "        lang = model['lang']\n",
    "        vocabs = model['vocabs']\n",
    "        vocabs_dev = model['vocabs_dev']\n",
    "        vocabs_test = model['vocabs_test']\n",
    "        \n",
    "        nb_classes = len(vocabs['LABELS'])\n",
    "        nb_classes_pos = len(vocabs['POS'])\n",
    "        nb_classes_morpho = len(vocabs['MORPHO'])\n",
    "        nb_classes_dist = 8\n",
    "        \n",
    "        unknown_idx = -1\n",
    "        unknown_lemma_idx = -1\n",
    "        unknown_morpho_idx = -1\n",
    "        if self.UNKNOWN_WORD in vocabs['WORDS']:\n",
    "            unknown_idx = vocabs['WORDS'].index(self.UNKNOWN_WORD)\n",
    "        if self.UNKNOWN_WORD in vocabs['LEMMA']:\n",
    "            unknown_lemma_idx = vocabs['LEMMA'].index(self.UNKNOWN_WORLD)\n",
    "        if self.UNKNOWN_WORD in vocabs['MORPHO']:\n",
    "            unknown_morpho_idx = vocabs['MORPHO'].index(self.UNKNOWN_WORLD)\n",
    "        \n",
    "        # align to vocabs indices (the ones known by the keras network)\n",
    "        if featureset == 'f1':\n",
    "            \"\"\"\n",
    "            2 S.0.POS\n",
    "            3 B.0.POS\n",
    "            4 DIST\"\"\"\n",
    "            cats_pos1 = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[2]]), num_classes=nb_classes_pos)\n",
    "            cats_pos2 = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[3]]), num_classes=nb_classes_pos)           \n",
    "            # dist is positive for arc eager\n",
    "            cats_dist = to_categorical(np.abs(X_test[4]), num_classes=nb_classes_dist)\n",
    "            \n",
    "            if use_forms:\n",
    "                w1 = vocabs_test['WORDS'][X_test[0]]\n",
    "                if w1 in vocabs['WORDS']:\n",
    "                    w1_idx = vocabs['WORDS'].index(w1)\n",
    "                else:\n",
    "                    w1_idx = unknown_idx\n",
    "                w2 = vocabs_test['WORDS'][X_test[1]]\n",
    "                if w2 in vocabs['WORDS']:\n",
    "                    w2_idx = vocabs['WORDS'].index(w2)\n",
    "                else:\n",
    "                    w2_idx = unknown_idx\n",
    "                    \n",
    "                X_test = np.asarray((w1_idx, w2_idx, cats_pos1, cats_pos2, cats_dist))\n",
    "            else:\n",
    "                X_test = np.asarray((cats_pos1, cats_pos2, cats_dist))\n",
    "            \n",
    "        elif featureset == 'f2':\n",
    "            \"\"\" \n",
    "            2 S.0.POS\n",
    "            3 S.0.LEMMA\n",
    "            4 S.0.MORPHO\n",
    "            5 S.-1.POS (f2, or S.1.POS for f3)\n",
    "            6 B.0.POS\n",
    "            7 B.0.LEMMA\n",
    "            8 B.0.MORPHO\n",
    "            9 B.-1.POS\n",
    "            10 B.1.POS\n",
    "            11 DIST\"\"\"        \n",
    "            cats_pos1    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[2]]), num_classes=nb_classes_pos)\n",
    "            morpho = vocabs_test['MORPHO'][X_test[4]]\n",
    "            if morpho in vocabs['MORPHO']:\n",
    "                m_idx = vocabs['MORPHO'].index(morpho)\n",
    "            else:\n",
    "                m_idx = unknown_morpho_idx\n",
    "            cats_morpho1 = to_categorical(m_idx, num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[5]]), num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[6]]), num_classes=nb_classes_pos)\n",
    "            morpho = vocabs_test['MORPHO'][X_test[8]]\n",
    "            if morpho in vocabs['MORPHO']:\n",
    "                m_idx = vocabs['MORPHO'].index(morpho)\n",
    "            else:\n",
    "                m_idx = unknown_morpho_idx            \n",
    "            cats_morpho2 = to_categorical(m_idx, num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[9]]), num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[10]]), num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_train[11]), num_classes=nb_classes_dist)\n",
    "            l1 = vocabs_test['LEMMA'][X_test[3]]\n",
    "            if l1 in vocabs['LEMMA']:\n",
    "                l1_idx = vocabs['LEMMA'].index(l1)\n",
    "            else:\n",
    "                l1_idx = unknown_lemma_idx\n",
    "            l2 = vocabs_test['LEMMA'][X_test[7]]\n",
    "            if l2 in vocabs['LEMMA']:\n",
    "                l2_idx = vocabs['LEMMA'].index(l2)\n",
    "            else:\n",
    "                l2_idx = unknown_lemma_idx\n",
    "            # lemmas (embeddings) are set at beginning of inputs\n",
    "            X_train = np.asarray((l1_idx, l2_idx, cats_pos1, cats_morpho1, cats_pos2, cats_pos3,\n",
    "                                       cat_morpho2, cats_pos4, cats_pos5, cats_dist))\n",
    "            \n",
    "        elif featureset == 'f3':\n",
    "            \"\"\" \n",
    "            2 S.0.POS\n",
    "            3 S.0.LEMMA\n",
    "            4 S.0.MORPHO\n",
    "            5 S.-1.POS    (S.1.POS does not exist)\n",
    "            6 B.0.POS\n",
    "            7 B.0.LEMMA\n",
    "            8 B.0.MORPHO\n",
    "            9 B.-2.POS    (added for f3)\n",
    "            10 B.-1.POS\n",
    "            11 B.1.POS\n",
    "            12 DIST\"\"\"  \n",
    "            cats_pos1    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[2]]), num_classes=nb_classes_pos)\n",
    "            morpho = vocabs_test['MORPHO'][X_test[4]]\n",
    "            if morpho in vocabs['MORPHO']:\n",
    "                m_idx = vocabs['MORPHO'].index(morpho)\n",
    "            else:\n",
    "                m_idx = unknown_morpho_idx\n",
    "            cats_morpho1 = to_categorical(m_idx, num_classes=nb_classes_morpho)\n",
    "            cats_pos2    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[5]]), num_classes=nb_classes_pos)\n",
    "            cats_pos3    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[6]]), num_classes=nb_classes_pos)\n",
    "            morpho = vocabs_test['MORPHO'][X_test[8]]\n",
    "            if morpho in vocabs['MORPHO']:\n",
    "                m_idx = vocabs['MORPHO'].index(morpho)\n",
    "            else:\n",
    "                m_idx = unknown_morpho_idx            \n",
    "            cats_morpho2 = to_categorical(m_idx, num_classes=nb_classes_morpho)\n",
    "            cats_pos4    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[9]]), num_classes=nb_classes_pos)\n",
    "            cats_pos5    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[10]]), num_classes=nb_classes_pos)\n",
    "            cats_pos6    = to_categorical(vocabs['POS'].index(vocabs_test['POS'][X_test[11]]), num_classes=nb_classes_pos)\n",
    "            cats_dist    = to_categorical(np.abs(X_train[12]), num_classes=nb_classes_dist)\n",
    "            l1 = vocabs_test['LEMMA'][X_test[3]]\n",
    "            if l1 in vocabs['LEMMA']:\n",
    "                l1_idx = vocabs['LEMMA'].index(l1)\n",
    "            else:\n",
    "                l1_idx = unknown_lemma_idx\n",
    "            l2 = vocabs_test['LEMMA'][X_test[7]]\n",
    "            if l2 in vocabs['LEMMA']:\n",
    "                l2_idx = vocabs['LEMMA'].index(l2)\n",
    "            else:\n",
    "                l2_idx = unknown_lemma_idx\n",
    "            # lemmas (embeddings) are set at beginning of inputs\n",
    "            X_test = np.asarray((l1_idx, l2_idx, cats_pos1, cats_morpho1, cats_pos2, cats_pos3,\n",
    "                                       cat_morpho2, cats_pos4, cats_pos5, cats_pos6, cats_dist))            \n",
    "            \n",
    "                \n",
    "        return X_test\n",
    "                                       \n",
    "    def get_label(self, y_pred):\n",
    "        y_pred_idx = np.argmax(y_pred)\n",
    "        return self.get_current_model()['vocabs']['LABELS'][y_pred_idx]\n",
    "    \n",
    "    def print_status(self):\n",
    "        print()\n",
    "        print(\"=== DependencyClassifier = max_vocab_size {max} unknown form {unk} ===\".format(max=self.MAX_VOCAB_SIZE,\n",
    "                                                                                             unk=self.UNKNOWN_WORD))\n",
    "        for model in self.models_:\n",
    "            print(\"   {name} -- {mod}\".format(name=model, mod=[\"{k}:{v},\" for k,v in self.models_[model].items()]))\n",
    "        for net in self.networks_:\n",
    "            print(net)\n",
    "            \n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle.load ../data\\f1_fr-train\n",
      "load_data: loaded X =  (742384, 6) , Y =  (742384,) , vocabs =  WORDS (42280), POS (20), LABELS (91), \n",
      "(742384, 6)\n"
     ]
    }
   ],
   "source": [
    "dm = DataManager('../')\n",
    "\n",
    "X_train, y_train, vocabs_train = dm.load_data('train', 'fr', 'f1')\n",
    "\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= fr === f1 ==========\n",
      "\n",
      "===== CREATING NEURAL NETWORK =====\n",
      "\n",
      "= Loading data...\n",
      "pickle.load ../data\\f1_fr-train\n",
      "load_data: loaded X =  (742384, 6) , Y =  (742384,) , vocabs =  WORDS (42280), POS (20), LABELS (91), \n",
      "pickle.load ../data\\f1_fr-dev\n",
      "load_data: loaded X =  (74492, 6) , Y =  (74492,) , vocabs =  WORDS (9277), POS (20), LABELS (75), \n",
      "pickle.load ../data\\f1_fr-test\n",
      "load_data: loaded X =  (20872, 6) , Y =  (20872,) , vocabs =  WORDS (3287), POS (20), LABELS (69), \n",
      "  ... loaded data in  1.3001785278320312\n",
      "pickle.load ../f1_fr-model.pkl\n",
      "= Pre-processing data ...\n",
      "preprocess_data: removing non-projective samples ...\n",
      "    before  (742384, 6) (742384,)\n",
      "    after  (742384, 6) (742384,)\n",
      "    before  (74492, 6) (74492,)\n",
      "    after  (74492, 6) (74492,)\n",
      "preprocess_data: merging and aligning LABEL vocabs from dev into train ...\n",
      "preprocess_data: merging and aligning LABEL vocabs from test into train ...\n",
      "preprocess_data: converting f1 features to one-hot encoding...\n",
      "preprocess_data: converting LABELs to one-hot encoding ...\n",
      "  ... preprocessed data in  2.528270959854126\n",
      "= Preprocessing embeddings ...\n",
      "preprocess_embeddings: loading original embeddings ...\n",
      "pickle.load ../cache\\wiki.fr.vec.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3545269b2006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0mdep_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-3545269b2006>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(epochs, max_vocab_size, unknown_word)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfeatureset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;34m'f1-forms'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;31m# embeddings have already been processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                 \u001b[0mdep_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment_vocabs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"  ... preprocessed embeddings in \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-cdc66f7e399d>\u001b[0m in \u001b[0;36mpreprocess_embeddings\u001b[1;34m(self, model_name, augment_vocabs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"preprocess_embeddings: loading original embeddings ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapax_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdm_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mfaligned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cache'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'-embeddings-forms-aligned.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-0f967fd51565>\u001b[0m in \u001b[0;36mload_embeddings\u001b[1;34m(self, lang, type)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_fasttext_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_fasttext_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-0f967fd51565>\u001b[0m in \u001b[0;36mload_fasttext_embeddings\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_ffname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m             \u001b[0mdata_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickle_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_ffname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Embedding for {lang} loaded from {fname}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_ffname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-0f967fd51565>\u001b[0m in \u001b[0;36mpickle_load\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import generate_data as aegen\n",
    "\n",
    "def STUB(class_mgr):\n",
    "    \"\"\"\n",
    "    Arc eager on keras model, using vocab_test (aligned to keras network embeddings), writing conllu file\n",
    "    as fname.conllu.\n",
    "    \"\"\"\n",
    "    keras_model = class_mgr.get_current_network()\n",
    "    \n",
    "    model = class_mgr.get_current_model()\n",
    "    featureset = model['featureset']\n",
    "    lang = model['lang']\n",
    "    \n",
    "    print(\"ArcEager: generate test results for {lang} / {featureset}\".format(lang=lang, featureset=featureset))\n",
    "    \n",
    "    if featureset == 'f1':\n",
    "        if model['use_forms']:\n",
    "            #X_test = [w1, w2, s0pos, b0pos, dist]\n",
    "            pass\n",
    "        else:\n",
    "            #X_test = [s0pos, b0pos, dist]\n",
    "            pass\n",
    "    elif featureset == 'f2':\n",
    "        # ...\n",
    "        pass\n",
    "    else:\n",
    "        # ...\n",
    "        pass\n",
    "    \n",
    "    # align to neural network input formats\n",
    "    #X_test = class_mgr.process_test_data(X_test)\n",
    "    #y_pred = keras_model.predict(X_test)\n",
    "    #y = keras_model.get_label(y_pred) # converts from one-hot to label string \"RIGHT_det\", using vocab\n",
    "    \n",
    "    # ...\n",
    "        \n",
    "    \n",
    "\n",
    "def main(epochs=10, max_vocab_size=100000, unknown_word='<UNK>'):\n",
    "\n",
    "    dm = DataManager('../')\n",
    "    # define a manager with hard limit of 100000 for vocabularies lengths\n",
    "    dep_classifier = DependencyClassifier(path='../', max_vocab_size=100000, unknown_word='<UNK>')\n",
    "    \n",
    "    test_info = {'fr': os.path.join(\"..\", \"UD_French-GSD\", \"fr_gsd-ud-test.conllu\"),       \n",
    "                 'nl' : os.path.join(\"..\", \"UD_Dutch-LassySmall\", \"nl_lassysmall-ud-test.conllu\") ,              \n",
    "                 'en' : os.path.join(\"..\", \"UD_English-LinES\", \"en_lines-ud-test.conllu\") ,                              \n",
    "                 'ja' : os.path.join(\"..\", \"UD_Japanese-GSD\", \"ja_gsd-ud-test.conllu\")\n",
    "                }\n",
    "\n",
    "    for lang in ['fr', 'nl', 'en', 'ja']:\n",
    "    \n",
    "        for featureset in ['f1', 'f1-forms', 'f2', 'f3']:\n",
    "            \n",
    "            test_results_fname = \"{featureset}_{lang}_results.conllu\"\n",
    "    \n",
    "            if os.path.isfile(test_results_fname):\n",
    "                print(\"{f} already exists, skipping this task\".format(f=test_results_fname))\n",
    "                break\n",
    "    \n",
    "            # === CREATE NEURAL NETWORK ===\n",
    "            print()\n",
    "            print(\"========= {lang} === {featureset} ==========\".format(lang=lang, featureset=featureset))\n",
    "            print()\n",
    "            print(\"===== CREATING NEURAL NETWORK =====\")\n",
    "            print()\n",
    "            t = time.time()\n",
    "            print(\"= Loading data...\")\n",
    "            if featureset is not 'f1-forms':\n",
    "                X_train, y_train, vocabs_train = dm.load_data('train', lang, featureset)\n",
    "                X_dev, y_dev, vocabs_dev = dm.load_data('dev', lang, featureset)\n",
    "                X_test, y_test, vocabs_test = dm.load_data('test', lang, featureset)\n",
    "\n",
    "            print(\"  ... loaded data in \", time.time() - t)\n",
    "\n",
    "            nb_classes = len(vocabs_train['LABELS'])\n",
    "\n",
    "            model_name = \"{featureset}_{lang}\".format(featureset=featureset, lang=lang)\n",
    "            \n",
    "            if featureset is 'f1-forms':\n",
    "                model_name = model_name + '-forms'\n",
    "\n",
    "            # create a TAL model for this combination\n",
    "            exists = dep_classifier.load_model(model_name)\n",
    "            if exists and dep_classifier.get_model_status(model_name):\n",
    "                print(\"==> Neural network for this model already prepared and trained\")\n",
    "                break\n",
    "            \n",
    "            if not exists:\n",
    "                dep_classifier.create_model(model_name, lang, featureset, featureset is 'f1-forms', \n",
    "                                            vocabs_train, vocabs_dev, vocabs_test)\n",
    "                dep_classifier.save_model(model_name)\n",
    "\n",
    "            if dep_classifier.get_model_test_status(model_name):\n",
    "                print(\"==> Test conllu file already generated, skipping...\")\n",
    "                break\n",
    "                \n",
    "            t = time.time()\n",
    "\n",
    "            print(\"= Pre-processing data ...\")\n",
    "            # preprocess X/Y data\n",
    "            X_train, y_train, X_dev, y_dev, X_test, y_test = dep_classifier.preprocess_data(model_name, \n",
    "                                                                                X_train, y_train, \n",
    "                                                                                X_dev, y_dev, \n",
    "                                                                                X_test, y_test)\n",
    "\n",
    "            print(\"  ... preprocessed data in \", time.time() - t)\n",
    "\n",
    "            t = time.time()\n",
    "            print(\"= Preprocessing embeddings ...\")\n",
    "            # load pretrained embeddings\n",
    "            if featureset is not 'f1-forms':\n",
    "                # embeddings have already been processed\n",
    "                dep_classifier.preprocess_embeddings(model_name, augment_vocabs=True)\n",
    "\n",
    "            print(\"  ... preprocessed embeddings in \", time.time() - t)\n",
    "\n",
    "            t = time.time()\n",
    "            print(\"= Creating neural network architecture ...\")\n",
    "            # create a classifier for this TAL model\n",
    "            network_name = \"{model}\".format(model=model_name)\n",
    "            existsnet = dep_classifier.load_network(network_name)\n",
    "            net = None\n",
    "            if not existsnet:\n",
    "                if dep_classifier.create_network(network_name, model_name, nb_classes=nb_classes, dropout=True):\n",
    "                    net = dep_classifier.get_network(network_name)\n",
    "                else:\n",
    "                    print(\"  ==> could not create network architecture for this model !!!\")\n",
    "\n",
    "            print(\"  ... created net in \", time.time() - t)\n",
    "\n",
    "            dep_classifier.print_status()\n",
    "            \n",
    "            print()\n",
    "            print(\"===== TRAINING NEURAL NETWORK =====\")\n",
    "            print()           \n",
    "        \n",
    "        \n",
    "            #@TODO currently if recalled network will be trained again for epochs if it already exists\n",
    "            print(\"= Training for {epochs} epochs ...\".format(epochs=epochs))\n",
    "            if net is not None:\n",
    "                history = net.fit([X_train[:, 0], X_train[:, 1], X_train[:, 2:X_train.shape[1]]], \n",
    "                  y_train,\n",
    "                  validation_data=([X_dev[:, 0], X_dev[:, 1], X_dev[:, 2:X_dev.shape[1]]], \n",
    "                                   y_dev),\n",
    "                  batch_size=128,\n",
    "                  epochs=epochs)\n",
    "            else:\n",
    "                print(\"problem : net is none\")\n",
    "            \n",
    "            # save the network\n",
    "            print(\"= Saving network architecture ...\")\n",
    "            dep_classifier.save_network(network_name)\n",
    "            \n",
    "            dep_classifier.set_model_done(model_name)\n",
    "            dep_classifier.save_model(model_name)            \n",
    "            \n",
    "        \n",
    "            #@TODO plot history ?\n",
    "            print(\"= Evaluating ...\")\n",
    "            results = net.evaluate([X_test[:, 0], X_test[:, 1], X_test[:, 2:X_test.shape[1]]], \n",
    "                                  y_test,\n",
    "                                  batch_size=128)\n",
    "            print(\"  ==> (temporary) results for {lang} / {featureset}: \", results)\n",
    "            \n",
    "            print()\n",
    "            print(\"===== GENERATING TEST RESULTS =====\")\n",
    "            #STUB(dep_classifier)\n",
    "            \n",
    "            print(\"= Calling Arc Eager with new oracle ...\")\n",
    "            if not get_model_test_status(model_name):\n",
    "                result_path = os.path.dirname(os.path.abspath(test_info[lang]))\n",
    "                result_name = os.path.join(result_path, test_results_fname)\n",
    "                aegen.create_conllu(test_info[lang], featureset[:2], result_name, oracle_=dep_classifier)\n",
    "            \n",
    "                print(\"= Setting model as fully done ...\")\n",
    "                # tag this combination as done and save it\n",
    "                dep_classifier.set_model_test_done(model_name)\n",
    "                dep_classifier.save_model(model_name)\n",
    "\n",
    "main(epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- il n'y a pas les mots dans les features (seulement les lemmes)\n",
    "\n",
    "- ajouter les mots uniquement pour l'expérience en plus\n",
    "\n",
    "- embeddings pour les lemmes: il faudrait ajouter tous les mots des embeddings ! (et pas les enlever lors de l'alignement)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
